{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y2AqTNRq6L6J"
   },
   "source": [
    "# Neural Network (Classification)\n",
    " \n",
    " - 3등 이내의 말을 찾는 모델을 구축 (연승식 베팅 가능)\n",
    " - 확률 형태의 결과값을 도출할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XLH0HfVf6L6l"
   },
   "source": [
    "## 전처리\n",
    " - 경기를 식별하기 위한 Groupid 부여\n",
    " - 결측치 있는 행은 제거\n",
    " - 말의 등급, 경기 등급, 습도 변수는 결측치가 많아 사용하지 않음\n",
    " - 말의 성별은 더미변수로 변환, 무게 및 속도 관련 변수는 표준화\n",
    " - 이전 3경기의 등수에 대한 정보는 평균으로 대체\n",
    " - 3등 이내 말의 rank는 1, 그 외는 0으로 재코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y63tQXfv6L6o"
   },
   "outputs": [],
   "source": [
    "result = pd.read_csv('race_result.csv')\n",
    "\n",
    "result = result.iloc[::-1]\n",
    "result = result.reset_index().drop('index', axis=1)\n",
    "\n",
    "# groupid 추가\n",
    "result['groupid'] = result['date'].apply(str) + result['round'].apply(str)\n",
    "idset = list(set(result['groupid']))\n",
    "give_ids = dict(list(zip(idset, list(range(len(idset))))))\n",
    "result['groupid'] = result['groupid'].apply(lambda x: give_ids[x])\n",
    "\n",
    "# 필요 변수만 선택\n",
    "result_df = result[ \n",
    "    ['date','round','groupid','rank', 'lane', 'sex', 'age','new_distance', 'prev1_rank', 'prev2_rank', 'prev3_rank', 'dandivi', 'yeondivi' ,\n",
    "      'jockey_w', 'raw_weight', 'weight_added' , 'prev1_velo', 'prev2_velo', 'prev3_velo',\n",
    "    'cure_in_1m', 'jprev1_rank', 'jprev2_rank', 'jprev3_rank']\n",
    "]\n",
    "\n",
    "# 결측치있는 raw 제거\n",
    "result_nonan = result_df.dropna()\n",
    "\n",
    "# sex 더미변수화\n",
    "sex = pd.get_dummies(result_nonan.sex)\n",
    "sex.columns = ['neut', 'male', 'female']\n",
    "result_nonan = result_nonan.drop('sex', axis=1)\n",
    "result_nonan = sex.join(result_nonan)\n",
    "\n",
    "# weight, velocity 표준화\n",
    "from sklearn import preprocessing\n",
    "standadized_result = result_nonan[['jockey_w', 'raw_weight', 'weight_added', 'prev1_velo', 'prev2_velo', 'prev3_velo']]\n",
    "scaler = preprocessing.StandardScaler().fit(standadized_result)\n",
    "preprocessing.StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "result_nonan[['jockey_w', 'raw_weight', 'weight_added', 'prev1_velo', 'prev2_velo', 'prev3_velo']] = scaler.transform(standadized_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k2Ks20dO6L6w"
   },
   "outputs": [],
   "source": [
    "# 1~3등: 1, 나머지: 0\n",
    "ranks_replace = {\n",
    "    'rank': {'1': 1, '2': 1, '3': 1, '4': 0, '5': 0, '6': 0, '7': 0, '8': 0, '9': 0, '10': 0, '11': 0, '12': 0, '13': 0, '14': 0, '15': 0, '16': 0, '실': None, '제': None, '중': None, '취': None},\n",
    "    'prev1_rank' : {'실': None, '제': None, '중': None, '취': None},\n",
    "    'prev2_rank' : {'실': None, '제': None, '중': None, '취': None},\n",
    "    'prev3_rank' : {'실': None, '제': None, '중': None, '취': None},\n",
    "    'jprev1_rank': {'실': None, '제': None, '중': None, '취': None}, \n",
    "    'jprev2_rank': {'실': None, '제': None, '중': None, '취': None},\n",
    "    'jprev3_rank': {'실': None, '제': None, '중': None, '취': None}\n",
    "}\n",
    "result_binary = result_nonan.replace(ranks_replace, inplace=False)\n",
    "result_binary = result_binary.dropna()\n",
    "\n",
    "cols = ['prev1_rank','prev2_rank','prev3_rank','jprev1_rank','jprev2_rank','jprev3_rank']\n",
    "result_binary[cols] = result_binary[cols].apply(pd.to_numeric, errors='coerce', axis=1)\n",
    "\n",
    "# prev_rank 평균으로 대체\n",
    "result_binary['prev_rank_mean'] = (result_binary['prev1_rank']+result_binary['prev2_rank']+result_binary['prev3_rank'])/3\n",
    "result_binary = result_binary.drop(['prev1_rank','prev2_rank','prev3_rank'], axis=1)\n",
    "\n",
    "rank = result_binary['rank']\n",
    "result_binary.drop(labels=['rank'], axis=1,inplace = True)\n",
    "result_binary.insert(0, 'rank', rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CTkArS6M6L61"
   },
   "source": [
    "## 학습데이터 분리\n",
    "- 1) test set: 2017년 9월 ~ 2018년 9월 데이터, train set: 그 외 기간\n",
    "- 2) test set: 2018년 3월 ~ 2018년 9월 데이터, train set: 그 외 기간\n",
    "--------------\n",
    "- 종속변수: rank 더미변수\n",
    "- 설명변수: 'lane', 'sex', 'age','new_distance', 'dandivi', 'yeondivi' , 'jockey_w', 'raw_weight', 'weight_added' , 'prev1_velo', 'prev2_velo', 'prev3_velo', 'cure_in_1m', 'jprev1_rank', 'jprev2_rank', 'jprev3_rank', 'prev_rank_mean'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J0l51borFfEJ"
   },
   "outputs": [],
   "source": [
    "# test set 1) 2017.09 ~ 2018.09   2) 2018.03 ~ 2018.09\n",
    "result_binary['year'] = pd.DatetimeIndex(result_binary['date']).year\n",
    "result_binary['month'] = pd.DatetimeIndex(result_binary['date']).month\n",
    "test_set1 = result_binary[((result_binary.year == 2017) & (result_binary.month >=9))|((result_binary.year == 2018) & (result_binary.month <=9))]\n",
    "train_set1 = result_binary[((result_binary.year <= 2017) & (result_binary.month < 9))|((result_binary.year >= 2018) & (result_binary.month > 9))]\n",
    "test_set2 = result_binary[(result_binary.year == 2018) & (result_binary.month >=3) & (result_binary.month <=9)]\n",
    "train_set2 = result_binary[((result_binary.year <= 2018) & (result_binary.month < 3))|((result_binary.year >= 2018) & (result_binary.month > 9))]\n",
    "\n",
    "# 모델에 불필요한 변수 drop\n",
    "test1 = test_set1.drop(['date','round','groupid','year','month'],axis=1)\n",
    "train1 = train_set1.drop(['date','round','groupid','year','month'],axis=1)\n",
    "test2 = test_set2.drop(['date','round','groupid','year','month'],axis=1)\n",
    "train2 = train_set2.drop(['date','round','groupid','year','month'],axis=1)\n",
    "\n",
    "x_train = train2.iloc[:, 1:]\n",
    "y_train = train2['rank'].astype(int)\n",
    "\n",
    "x_test = test2.iloc[:, 1:]\n",
    "y_test = test2['rank'].astype(int)\n",
    "\n",
    "print_shape((x_train, y_train, x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neut</th>\n",
       "      <th>male</th>\n",
       "      <th>female</th>\n",
       "      <th>lane</th>\n",
       "      <th>age</th>\n",
       "      <th>new_distance</th>\n",
       "      <th>dandivi</th>\n",
       "      <th>yeondivi</th>\n",
       "      <th>jockey_w</th>\n",
       "      <th>raw_weight</th>\n",
       "      <th>...</th>\n",
       "      <th>prev1_velo</th>\n",
       "      <th>prev2_velo</th>\n",
       "      <th>prev3_velo</th>\n",
       "      <th>cure_in_1m</th>\n",
       "      <th>jprev1_rank</th>\n",
       "      <th>jprev2_rank</th>\n",
       "      <th>jprev3_rank</th>\n",
       "      <th>prev_velo_avg</th>\n",
       "      <th>prev_rank_avg</th>\n",
       "      <th>prev_rank_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>40.2</td>\n",
       "      <td>8.3</td>\n",
       "      <td>-1.385726</td>\n",
       "      <td>-0.190196</td>\n",
       "      <td>...</td>\n",
       "      <td>1.454755</td>\n",
       "      <td>1.701469</td>\n",
       "      <td>-2.634194</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>15.595166</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>68.3</td>\n",
       "      <td>11.8</td>\n",
       "      <td>0.652003</td>\n",
       "      <td>0.194163</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040870</td>\n",
       "      <td>-1.325378</td>\n",
       "      <td>1.156693</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>15.497170</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6.8</td>\n",
       "      <td>2.2</td>\n",
       "      <td>-0.876294</td>\n",
       "      <td>-0.432948</td>\n",
       "      <td>...</td>\n",
       "      <td>1.643153</td>\n",
       "      <td>0.862988</td>\n",
       "      <td>0.551053</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>15.963258</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>66.4</td>\n",
       "      <td>10.6</td>\n",
       "      <td>-0.876294</td>\n",
       "      <td>0.720127</td>\n",
       "      <td>...</td>\n",
       "      <td>0.210016</td>\n",
       "      <td>0.506775</td>\n",
       "      <td>0.816098</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>15.739252</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>23.6</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.161435</td>\n",
       "      <td>0.093016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.727342</td>\n",
       "      <td>0.859280</td>\n",
       "      <td>2.010973</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>16.040071</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   neut  male  female  lane  age  new_distance  dandivi  yeondivi  jockey_w  \\\n",
       "0     0     0       1    11    4             0     40.2       8.3 -1.385726   \n",
       "1     0     0       1     6    5             1     68.3      11.8  0.652003   \n",
       "2     0     0       1     8    5             1      6.8       2.2 -0.876294   \n",
       "3     0     1       0     4    4             1     66.4      10.6 -0.876294   \n",
       "4     1     0       0     9    4             1     23.6       3.8  1.161435   \n",
       "\n",
       "   raw_weight       ...        prev1_velo  prev2_velo  prev3_velo  cure_in_1m  \\\n",
       "0   -0.190196       ...          1.454755    1.701469   -2.634194           0   \n",
       "1    0.194163       ...          0.040870   -1.325378    1.156693           1   \n",
       "2   -0.432948       ...          1.643153    0.862988    0.551053           1   \n",
       "3    0.720127       ...          0.210016    0.506775    0.816098           0   \n",
       "4    0.093016       ...          0.727342    0.859280    2.010973           0   \n",
       "\n",
       "   jprev1_rank  jprev2_rank  jprev3_rank  prev_velo_avg  prev_rank_avg  \\\n",
       "0            2            6            2      15.595166            2.0   \n",
       "1            5            7            4      15.497170            1.0   \n",
       "2            1            2            5      15.963258           11.0   \n",
       "3           10            4            3      15.739252            3.0   \n",
       "4            4            4            6      16.040071           12.0   \n",
       "\n",
       "   prev_rank_mean  \n",
       "0        6.000000  \n",
       "1        6.000000  \n",
       "2        4.666667  \n",
       "3        5.333333  \n",
       "4        3.333333  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dL_BBwscI-7G"
   },
   "source": [
    "## Neural Network 모델 구축\n",
    "### 모델 구성하기\n",
    "- Python에서 신경망 모델을 쉽게 구축하게 해주는 딥러닝 라이브러리 Keras 사용\n",
    "- `Batch Normalization`으로 모델 성능 개선\n",
    "- `Dropout`으로 overfitting 방지\n",
    "- 활성함수로 `sigmoid` 함수를 사용해 0과 1사이의 결과값 도출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KmDbpg6V-TQZ"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import keras_metrics\n",
    "import keras\n",
    "import random\n",
    "from sklearn.utils import class_weight\n",
    "from keras import callbacks\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "xavier = keras.initializers.glorot_normal(seed=None)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=21, kernel_initializer=xavier, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout (0.2))\n",
    "model.add(Dense(16, input_dim=21, kernel_initializer=xavier, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout (0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 학습시키기\n",
    "- 성능이 가장 좋다고 알려진 adam optimizer 사용\n",
    "- 종속변수가 불균형 데이터 (1~3등 비율이 전체 대비 적음) -> class_weight 옵션을 통해 해결 \n",
    "\n",
    "### 학습과정 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 18684
    },
    "colab_type": "code",
    "id": "C5SwRU19-coe",
    "outputId": "1feb6a13-e181-4131-aef7-70e159fe90ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "8081/8081 [==============================] - 1s 156us/step - loss: 0.5300 - acc: 0.7342 - recall: 0.3420 - precision: 0.5067 - f1score: 0.3832\n",
      "Epoch 2/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4983 - acc: 0.7536 - recall: 0.3045 - precision: 0.5461 - f1score: 0.3728\n",
      "Epoch 3/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4896 - acc: 0.7578 - recall: 0.2905 - precision: 0.5760 - f1score: 0.3637\n",
      "Epoch 4/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4818 - acc: 0.7635 - recall: 0.3163 - precision: 0.5889 - f1score: 0.3920\n",
      "Epoch 5/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4829 - acc: 0.7603 - recall: 0.3164 - precision: 0.5812 - f1score: 0.3887\n",
      "Epoch 6/500\n",
      "8081/8081 [==============================] - 0s 51us/step - loss: 0.4777 - acc: 0.7714 - recall: 0.3350 - precision: 0.6103 - f1score: 0.4128\n",
      "Epoch 7/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4745 - acc: 0.7703 - recall: 0.3402 - precision: 0.5964 - f1score: 0.4177\n",
      "Epoch 8/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4730 - acc: 0.7733 - recall: 0.3686 - precision: 0.6056 - f1score: 0.4408\n",
      "Epoch 9/500\n",
      "8081/8081 [==============================] - 0s 56us/step - loss: 0.4725 - acc: 0.7718 - recall: 0.3425 - precision: 0.6133 - f1score: 0.4219\n",
      "Epoch 10/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4756 - acc: 0.7726 - recall: 0.3466 - precision: 0.6193 - f1score: 0.4239\n",
      "Epoch 11/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4700 - acc: 0.7771 - recall: 0.3792 - precision: 0.6229 - f1score: 0.4523\n",
      "Epoch 12/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4745 - acc: 0.7759 - recall: 0.3599 - precision: 0.6174 - f1score: 0.4375\n",
      "Epoch 13/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4742 - acc: 0.7768 - recall: 0.3594 - precision: 0.6257 - f1score: 0.4382\n",
      "Epoch 14/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4724 - acc: 0.7786 - recall: 0.3664 - precision: 0.6279 - f1score: 0.4449\n",
      "Epoch 15/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4710 - acc: 0.7771 - recall: 0.3663 - precision: 0.6315 - f1score: 0.4465\n",
      "Epoch 16/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4693 - acc: 0.7780 - recall: 0.3775 - precision: 0.6208 - f1score: 0.4543\n",
      "Epoch 17/500\n",
      "8081/8081 [==============================] - 0s 51us/step - loss: 0.4708 - acc: 0.7730 - recall: 0.3596 - precision: 0.6136 - f1score: 0.4342\n",
      "Epoch 18/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4703 - acc: 0.7738 - recall: 0.3613 - precision: 0.6115 - f1score: 0.4372\n",
      "Epoch 19/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4694 - acc: 0.7771 - recall: 0.3691 - precision: 0.6174 - f1score: 0.4443\n",
      "Epoch 20/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4712 - acc: 0.7801 - recall: 0.3682 - precision: 0.6469 - f1score: 0.4491\n",
      "Epoch 21/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4674 - acc: 0.7781 - recall: 0.3694 - precision: 0.6220 - f1score: 0.4476\n",
      "Epoch 22/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4676 - acc: 0.7826 - recall: 0.3668 - precision: 0.6442 - f1score: 0.4523\n",
      "Epoch 23/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4730 - acc: 0.7795 - recall: 0.3705 - precision: 0.6279 - f1score: 0.4485\n",
      "Epoch 24/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4659 - acc: 0.7848 - recall: 0.3761 - precision: 0.6577 - f1score: 0.4638\n",
      "Epoch 25/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4718 - acc: 0.7759 - recall: 0.3571 - precision: 0.6204 - f1score: 0.4388\n",
      "Epoch 26/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4686 - acc: 0.7787 - recall: 0.3527 - precision: 0.6402 - f1score: 0.4382\n",
      "Epoch 27/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4730 - acc: 0.7776 - recall: 0.3690 - precision: 0.6266 - f1score: 0.4470\n",
      "Epoch 28/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4662 - acc: 0.7785 - recall: 0.3541 - precision: 0.6326 - f1score: 0.4393\n",
      "Epoch 29/500\n",
      "8081/8081 [==============================] - 0s 51us/step - loss: 0.4688 - acc: 0.7760 - recall: 0.3722 - precision: 0.6253 - f1score: 0.4488\n",
      "Epoch 30/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4682 - acc: 0.7799 - recall: 0.3645 - precision: 0.6467 - f1score: 0.4457\n",
      "Epoch 31/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4697 - acc: 0.7756 - recall: 0.3581 - precision: 0.6237 - f1score: 0.4351\n",
      "Epoch 32/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4672 - acc: 0.7805 - recall: 0.3664 - precision: 0.6433 - f1score: 0.4495\n",
      "Epoch 33/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4692 - acc: 0.7787 - recall: 0.3662 - precision: 0.6339 - f1score: 0.4459\n",
      "Epoch 34/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4673 - acc: 0.7787 - recall: 0.3567 - precision: 0.6402 - f1score: 0.4391\n",
      "Epoch 35/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4671 - acc: 0.7806 - recall: 0.3685 - precision: 0.6412 - f1score: 0.4509\n",
      "Epoch 36/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4677 - acc: 0.7774 - recall: 0.3598 - precision: 0.6220 - f1score: 0.4409\n",
      "Epoch 37/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4664 - acc: 0.7796 - recall: 0.3812 - precision: 0.6314 - f1score: 0.4574\n",
      "Epoch 38/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4654 - acc: 0.7816 - recall: 0.3806 - precision: 0.6421 - f1score: 0.4606\n",
      "Epoch 39/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4699 - acc: 0.7782 - recall: 0.3492 - precision: 0.6408 - f1score: 0.4306\n",
      "Epoch 40/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4658 - acc: 0.7847 - recall: 0.3745 - precision: 0.6552 - f1score: 0.4594\n",
      "Epoch 41/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4688 - acc: 0.7771 - recall: 0.3516 - precision: 0.6262 - f1score: 0.4325\n",
      "Epoch 42/500\n",
      "8081/8081 [==============================] - 0s 57us/step - loss: 0.4672 - acc: 0.7818 - recall: 0.3655 - precision: 0.6465 - f1score: 0.4502\n",
      "Epoch 43/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4689 - acc: 0.7768 - recall: 0.3639 - precision: 0.6238 - f1score: 0.4428\n",
      "Epoch 44/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4622 - acc: 0.7834 - recall: 0.3937 - precision: 0.6474 - f1score: 0.4722\n",
      "Epoch 45/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4677 - acc: 0.7776 - recall: 0.3677 - precision: 0.6277 - f1score: 0.4456\n",
      "Epoch 46/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4635 - acc: 0.7813 - recall: 0.3865 - precision: 0.6324 - f1score: 0.4623\n",
      "Epoch 47/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4653 - acc: 0.7781 - recall: 0.3735 - precision: 0.6293 - f1score: 0.4510\n",
      "Epoch 48/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4658 - acc: 0.7816 - recall: 0.3748 - precision: 0.6431 - f1score: 0.4555\n",
      "Epoch 49/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4634 - acc: 0.7816 - recall: 0.3832 - precision: 0.6449 - f1score: 0.4631\n",
      "Epoch 50/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4645 - acc: 0.7794 - recall: 0.3736 - precision: 0.6377 - f1score: 0.4540\n",
      "Epoch 51/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4679 - acc: 0.7803 - recall: 0.3696 - precision: 0.6402 - f1score: 0.4488\n",
      "Epoch 52/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4660 - acc: 0.7786 - recall: 0.3496 - precision: 0.6377 - f1score: 0.4374\n",
      "Epoch 53/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4647 - acc: 0.7800 - recall: 0.3639 - precision: 0.6285 - f1score: 0.4463\n",
      "Epoch 54/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4612 - acc: 0.7820 - recall: 0.3863 - precision: 0.6375 - f1score: 0.4654\n",
      "Epoch 55/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4653 - acc: 0.7799 - recall: 0.3800 - precision: 0.6284 - f1score: 0.4544\n",
      "Epoch 56/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4655 - acc: 0.7832 - recall: 0.3754 - precision: 0.6481 - f1score: 0.4553\n",
      "Epoch 57/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4636 - acc: 0.7818 - recall: 0.3833 - precision: 0.6388 - f1score: 0.4609\n",
      "Epoch 58/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4681 - acc: 0.7750 - recall: 0.3644 - precision: 0.6259 - f1score: 0.4390\n",
      "Epoch 59/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4658 - acc: 0.7749 - recall: 0.3800 - precision: 0.6131 - f1score: 0.4503\n",
      "Epoch 60/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4656 - acc: 0.7778 - recall: 0.3530 - precision: 0.6403 - f1score: 0.4359\n",
      "Epoch 61/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4644 - acc: 0.7790 - recall: 0.3711 - precision: 0.6300 - f1score: 0.4485\n",
      "Epoch 62/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4656 - acc: 0.7836 - recall: 0.3826 - precision: 0.6421 - f1score: 0.4622\n",
      "Epoch 63/500\n",
      "8081/8081 [==============================] - 0s 51us/step - loss: 0.4674 - acc: 0.7771 - recall: 0.3826 - precision: 0.6203 - f1score: 0.4528\n",
      "Epoch 64/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4677 - acc: 0.7828 - recall: 0.3736 - precision: 0.6538 - f1score: 0.4546\n",
      "Epoch 65/500\n",
      "8081/8081 [==============================] - 0s 51us/step - loss: 0.4656 - acc: 0.7786 - recall: 0.3710 - precision: 0.6308 - f1score: 0.4482\n",
      "Epoch 66/500\n",
      "8081/8081 [==============================] - 0s 51us/step - loss: 0.4678 - acc: 0.7780 - recall: 0.3508 - precision: 0.6411 - f1score: 0.4352\n",
      "Epoch 67/500\n",
      "8081/8081 [==============================] - 0s 51us/step - loss: 0.4676 - acc: 0.7782 - recall: 0.3562 - precision: 0.6298 - f1score: 0.4379\n",
      "Epoch 68/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4672 - acc: 0.7733 - recall: 0.3331 - precision: 0.6284 - f1score: 0.4174\n",
      "Epoch 69/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4648 - acc: 0.7787 - recall: 0.3566 - precision: 0.6291 - f1score: 0.4394\n",
      "Epoch 70/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4660 - acc: 0.7802 - recall: 0.3783 - precision: 0.6354 - f1score: 0.4532\n",
      "Epoch 71/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4655 - acc: 0.7817 - recall: 0.3683 - precision: 0.6363 - f1score: 0.4508\n",
      "Epoch 72/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4652 - acc: 0.7831 - recall: 0.3878 - precision: 0.6427 - f1score: 0.4653\n",
      "Epoch 73/500\n",
      "8081/8081 [==============================] - 0s 51us/step - loss: 0.4664 - acc: 0.7839 - recall: 0.3934 - precision: 0.6449 - f1score: 0.4701\n",
      "Epoch 74/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4612 - acc: 0.7842 - recall: 0.3597 - precision: 0.6575 - f1score: 0.4480\n",
      "Epoch 75/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4644 - acc: 0.7852 - recall: 0.3997 - precision: 0.6438 - f1score: 0.4735\n",
      "Epoch 76/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4623 - acc: 0.7812 - recall: 0.3765 - precision: 0.6343 - f1score: 0.4565\n",
      "Epoch 77/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4642 - acc: 0.7811 - recall: 0.3847 - precision: 0.6374 - f1score: 0.4586\n",
      "Epoch 78/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4629 - acc: 0.7870 - recall: 0.3889 - precision: 0.6577 - f1score: 0.4728\n",
      "Epoch 79/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4658 - acc: 0.7800 - recall: 0.3766 - precision: 0.6293 - f1score: 0.4561\n",
      "Epoch 80/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4650 - acc: 0.7823 - recall: 0.3781 - precision: 0.6402 - f1score: 0.4598\n",
      "Epoch 81/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4637 - acc: 0.7844 - recall: 0.3962 - precision: 0.6404 - f1score: 0.4724\n",
      "Epoch 82/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4632 - acc: 0.7817 - recall: 0.3881 - precision: 0.6334 - f1score: 0.4615\n",
      "Epoch 83/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4645 - acc: 0.7822 - recall: 0.3721 - precision: 0.6370 - f1score: 0.4534\n",
      "Epoch 84/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4654 - acc: 0.7803 - recall: 0.3600 - precision: 0.6428 - f1score: 0.4421\n",
      "Epoch 85/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4607 - acc: 0.7846 - recall: 0.3806 - precision: 0.6572 - f1score: 0.4661\n",
      "Epoch 86/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4657 - acc: 0.7818 - recall: 0.3767 - precision: 0.6511 - f1score: 0.4571\n",
      "Epoch 87/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4653 - acc: 0.7796 - recall: 0.3592 - precision: 0.6333 - f1score: 0.4409\n",
      "Epoch 88/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4633 - acc: 0.7805 - recall: 0.3781 - precision: 0.6408 - f1score: 0.4584\n",
      "Epoch 89/500\n",
      "8081/8081 [==============================] - 0s 55us/step - loss: 0.4632 - acc: 0.7776 - recall: 0.3634 - precision: 0.6317 - f1score: 0.4429\n",
      "Epoch 90/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4624 - acc: 0.7815 - recall: 0.3680 - precision: 0.6398 - f1score: 0.4514\n",
      "Epoch 91/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4638 - acc: 0.7796 - recall: 0.3633 - precision: 0.6392 - f1score: 0.4457\n",
      "Epoch 92/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4645 - acc: 0.7790 - recall: 0.3506 - precision: 0.6346 - f1score: 0.4362\n",
      "Epoch 93/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4630 - acc: 0.7781 - recall: 0.3594 - precision: 0.6324 - f1score: 0.4406\n",
      "Epoch 94/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4622 - acc: 0.7846 - recall: 0.3842 - precision: 0.6500 - f1score: 0.4651\n",
      "Epoch 95/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4648 - acc: 0.7795 - recall: 0.3818 - precision: 0.6258 - f1score: 0.4572\n",
      "Epoch 96/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4639 - acc: 0.7785 - recall: 0.3643 - precision: 0.6455 - f1score: 0.4458\n",
      "Epoch 97/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4603 - acc: 0.7834 - recall: 0.3923 - precision: 0.6448 - f1score: 0.4683\n",
      "Epoch 98/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4619 - acc: 0.7847 - recall: 0.3961 - precision: 0.6401 - f1score: 0.4721\n",
      "Epoch 99/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4610 - acc: 0.7848 - recall: 0.3906 - precision: 0.6486 - f1score: 0.4689\n",
      "Epoch 100/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4638 - acc: 0.7776 - recall: 0.3649 - precision: 0.6348 - f1score: 0.4409\n",
      "Epoch 101/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4635 - acc: 0.7773 - recall: 0.3636 - precision: 0.6353 - f1score: 0.4385\n",
      "Epoch 102/500\n",
      "8081/8081 [==============================] - 0s 51us/step - loss: 0.4600 - acc: 0.7801 - recall: 0.3777 - precision: 0.6337 - f1score: 0.4538\n",
      "Epoch 103/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4625 - acc: 0.7816 - recall: 0.3648 - precision: 0.6438 - f1score: 0.4492\n",
      "Epoch 104/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4630 - acc: 0.7822 - recall: 0.3647 - precision: 0.6465 - f1score: 0.4512\n",
      "Epoch 105/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4661 - acc: 0.7774 - recall: 0.3595 - precision: 0.6266 - f1score: 0.4392\n",
      "Epoch 106/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4624 - acc: 0.7771 - recall: 0.3690 - precision: 0.6300 - f1score: 0.4462\n",
      "Epoch 107/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4680 - acc: 0.7822 - recall: 0.3834 - precision: 0.6499 - f1score: 0.4629\n",
      "Epoch 108/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4611 - acc: 0.7853 - recall: 0.3797 - precision: 0.6467 - f1score: 0.4642\n",
      "Epoch 109/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4660 - acc: 0.7800 - recall: 0.3814 - precision: 0.6305 - f1score: 0.4559\n",
      "Epoch 110/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4619 - acc: 0.7847 - recall: 0.3825 - precision: 0.6483 - f1score: 0.4651\n",
      "Epoch 111/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4634 - acc: 0.7810 - recall: 0.3706 - precision: 0.6476 - f1score: 0.4511\n",
      "Epoch 112/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4645 - acc: 0.7805 - recall: 0.3783 - precision: 0.6374 - f1score: 0.4557\n",
      "Epoch 113/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4630 - acc: 0.7862 - recall: 0.3820 - precision: 0.6565 - f1score: 0.4670\n",
      "Epoch 114/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4626 - acc: 0.7821 - recall: 0.3665 - precision: 0.6415 - f1score: 0.4515\n",
      "Epoch 115/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4609 - acc: 0.7822 - recall: 0.3910 - precision: 0.6427 - f1score: 0.4673\n",
      "Epoch 116/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4612 - acc: 0.7848 - recall: 0.3742 - precision: 0.6535 - f1score: 0.4599\n",
      "Epoch 117/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4621 - acc: 0.7828 - recall: 0.3696 - precision: 0.6465 - f1score: 0.4554\n",
      "Epoch 118/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4636 - acc: 0.7821 - recall: 0.3701 - precision: 0.6434 - f1score: 0.4498\n",
      "Epoch 119/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4626 - acc: 0.7799 - recall: 0.3704 - precision: 0.6402 - f1score: 0.4504\n",
      "Epoch 120/500\n",
      "8081/8081 [==============================] - 0s 51us/step - loss: 0.4614 - acc: 0.7826 - recall: 0.3923 - precision: 0.6399 - f1score: 0.4663\n",
      "Epoch 121/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4653 - acc: 0.7810 - recall: 0.3733 - precision: 0.6348 - f1score: 0.4505\n",
      "Epoch 122/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4653 - acc: 0.7801 - recall: 0.3598 - precision: 0.6403 - f1score: 0.4450\n",
      "Epoch 123/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4642 - acc: 0.7821 - recall: 0.3702 - precision: 0.6508 - f1score: 0.4531\n",
      "Epoch 124/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4625 - acc: 0.7825 - recall: 0.3721 - precision: 0.6476 - f1score: 0.4525\n",
      "Epoch 125/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4593 - acc: 0.7857 - recall: 0.3820 - precision: 0.6567 - f1score: 0.4678\n",
      "Epoch 126/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4617 - acc: 0.7851 - recall: 0.3776 - precision: 0.6474 - f1score: 0.4590\n",
      "Epoch 127/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4608 - acc: 0.7829 - recall: 0.3702 - precision: 0.6535 - f1score: 0.4553\n",
      "Epoch 128/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4638 - acc: 0.7800 - recall: 0.3743 - precision: 0.6301 - f1score: 0.4503\n",
      "Epoch 129/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4596 - acc: 0.7857 - recall: 0.3725 - precision: 0.6562 - f1score: 0.4586\n",
      "Epoch 130/500\n",
      "8081/8081 [==============================] - 0s 51us/step - loss: 0.4607 - acc: 0.7828 - recall: 0.3727 - precision: 0.6397 - f1score: 0.4550\n",
      "Epoch 131/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4610 - acc: 0.7813 - recall: 0.3761 - precision: 0.6374 - f1score: 0.4532\n",
      "Epoch 132/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4611 - acc: 0.7817 - recall: 0.3727 - precision: 0.6438 - f1score: 0.4524\n",
      "Epoch 133/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4587 - acc: 0.7848 - recall: 0.3727 - precision: 0.6612 - f1score: 0.4581\n",
      "Epoch 134/500\n",
      "8081/8081 [==============================] - 0s 51us/step - loss: 0.4620 - acc: 0.7785 - recall: 0.3795 - precision: 0.6254 - f1score: 0.4553\n",
      "Epoch 135/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4642 - acc: 0.7820 - recall: 0.3758 - precision: 0.6365 - f1score: 0.4565\n",
      "Epoch 136/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4648 - acc: 0.7759 - recall: 0.3662 - precision: 0.6259 - f1score: 0.4410\n",
      "Epoch 137/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4602 - acc: 0.7870 - recall: 0.3791 - precision: 0.6551 - f1score: 0.4637\n",
      "Epoch 138/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4613 - acc: 0.7836 - recall: 0.3795 - precision: 0.6521 - f1score: 0.4600\n",
      "Epoch 139/500\n",
      "8081/8081 [==============================] - 0s 55us/step - loss: 0.4636 - acc: 0.7766 - recall: 0.3727 - precision: 0.6259 - f1score: 0.4482\n",
      "Epoch 140/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4617 - acc: 0.7846 - recall: 0.3739 - precision: 0.6574 - f1score: 0.4570\n",
      "Epoch 141/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4651 - acc: 0.7795 - recall: 0.3641 - precision: 0.6319 - f1score: 0.4431\n",
      "Epoch 142/500\n",
      "8081/8081 [==============================] - 0s 55us/step - loss: 0.4634 - acc: 0.7778 - recall: 0.3819 - precision: 0.6317 - f1score: 0.4550\n",
      "Epoch 143/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4618 - acc: 0.7821 - recall: 0.3819 - precision: 0.6400 - f1score: 0.4592\n",
      "Epoch 144/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4616 - acc: 0.7829 - recall: 0.3691 - precision: 0.6562 - f1score: 0.4548\n",
      "Epoch 145/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4639 - acc: 0.7822 - recall: 0.3643 - precision: 0.6555 - f1score: 0.4503\n",
      "Epoch 146/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4628 - acc: 0.7821 - recall: 0.3754 - precision: 0.6361 - f1score: 0.4555\n",
      "Epoch 147/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4603 - acc: 0.7828 - recall: 0.3698 - precision: 0.6482 - f1score: 0.4530\n",
      "Epoch 148/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4615 - acc: 0.7831 - recall: 0.3691 - precision: 0.6443 - f1score: 0.4553\n",
      "Epoch 149/500\n",
      "8081/8081 [==============================] - 0s 56us/step - loss: 0.4638 - acc: 0.7842 - recall: 0.3785 - precision: 0.6497 - f1score: 0.4608\n",
      "Epoch 150/500\n",
      "8081/8081 [==============================] - 0s 56us/step - loss: 0.4605 - acc: 0.7846 - recall: 0.3811 - precision: 0.6576 - f1score: 0.4629\n",
      "Epoch 151/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4623 - acc: 0.7841 - recall: 0.3776 - precision: 0.6503 - f1score: 0.4614\n",
      "Epoch 152/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4641 - acc: 0.7800 - recall: 0.3595 - precision: 0.6519 - f1score: 0.4415\n",
      "Epoch 153/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4628 - acc: 0.7799 - recall: 0.3661 - precision: 0.6439 - f1score: 0.4472\n",
      "Epoch 154/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4611 - acc: 0.7831 - recall: 0.3741 - precision: 0.6584 - f1score: 0.4594\n",
      "Epoch 155/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4603 - acc: 0.7844 - recall: 0.3650 - precision: 0.6576 - f1score: 0.4541\n",
      "Epoch 156/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4629 - acc: 0.7799 - recall: 0.3870 - precision: 0.6306 - f1score: 0.4593\n",
      "Epoch 157/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4604 - acc: 0.7841 - recall: 0.3604 - precision: 0.6545 - f1score: 0.4450\n",
      "Epoch 158/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4626 - acc: 0.7778 - recall: 0.3767 - precision: 0.6268 - f1score: 0.4519\n",
      "Epoch 159/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4609 - acc: 0.7846 - recall: 0.3914 - precision: 0.6453 - f1score: 0.4707\n",
      "Epoch 160/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4621 - acc: 0.7817 - recall: 0.3551 - precision: 0.6476 - f1score: 0.4414\n",
      "Epoch 161/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4626 - acc: 0.7801 - recall: 0.3722 - precision: 0.6373 - f1score: 0.4514\n",
      "Epoch 162/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4622 - acc: 0.7816 - recall: 0.3659 - precision: 0.6497 - f1score: 0.4488\n",
      "Epoch 163/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4614 - acc: 0.7831 - recall: 0.3786 - precision: 0.6540 - f1score: 0.4566\n",
      "Epoch 164/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4591 - acc: 0.7832 - recall: 0.3730 - precision: 0.6503 - f1score: 0.4570\n",
      "Epoch 165/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4616 - acc: 0.7843 - recall: 0.3804 - precision: 0.6493 - f1score: 0.4619\n",
      "Epoch 166/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4620 - acc: 0.7813 - recall: 0.3761 - precision: 0.6380 - f1score: 0.4556\n",
      "Epoch 167/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4641 - acc: 0.7806 - recall: 0.3644 - precision: 0.6511 - f1score: 0.4481\n",
      "Epoch 168/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4618 - acc: 0.7802 - recall: 0.3738 - precision: 0.6352 - f1score: 0.4501\n",
      "Epoch 169/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4634 - acc: 0.7759 - recall: 0.3557 - precision: 0.6260 - f1score: 0.4332\n",
      "Epoch 170/500\n",
      "8081/8081 [==============================] - 0s 55us/step - loss: 0.4628 - acc: 0.7778 - recall: 0.3676 - precision: 0.6289 - f1score: 0.4462\n",
      "Epoch 171/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4629 - acc: 0.7782 - recall: 0.3774 - precision: 0.6313 - f1score: 0.4561\n",
      "Epoch 172/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4647 - acc: 0.7810 - recall: 0.3682 - precision: 0.6374 - f1score: 0.4501\n",
      "Epoch 173/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4613 - acc: 0.7864 - recall: 0.3796 - precision: 0.6537 - f1score: 0.4632\n",
      "Epoch 174/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4629 - acc: 0.7800 - recall: 0.3800 - precision: 0.6351 - f1score: 0.4560\n",
      "Epoch 175/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4588 - acc: 0.7853 - recall: 0.3796 - precision: 0.6561 - f1score: 0.4651\n",
      "Epoch 176/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4630 - acc: 0.7799 - recall: 0.3584 - precision: 0.6416 - f1score: 0.4420\n",
      "Epoch 177/500\n",
      "8081/8081 [==============================] - 0s 55us/step - loss: 0.4597 - acc: 0.7853 - recall: 0.3882 - precision: 0.6482 - f1score: 0.4679\n",
      "Epoch 178/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4631 - acc: 0.7816 - recall: 0.3715 - precision: 0.6444 - f1score: 0.4534\n",
      "Epoch 179/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4605 - acc: 0.7831 - recall: 0.3771 - precision: 0.6528 - f1score: 0.4603\n",
      "Epoch 180/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4607 - acc: 0.7797 - recall: 0.3766 - precision: 0.6351 - f1score: 0.4535\n",
      "Epoch 181/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4607 - acc: 0.7811 - recall: 0.3776 - precision: 0.6411 - f1score: 0.4589\n",
      "Epoch 182/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4603 - acc: 0.7829 - recall: 0.3845 - precision: 0.6449 - f1score: 0.4625\n",
      "Epoch 183/500\n",
      "8081/8081 [==============================] - 0s 56us/step - loss: 0.4628 - acc: 0.7813 - recall: 0.3702 - precision: 0.6354 - f1score: 0.4521\n",
      "Epoch 184/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4620 - acc: 0.7837 - recall: 0.3686 - precision: 0.6569 - f1score: 0.4567\n",
      "Epoch 185/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4634 - acc: 0.7816 - recall: 0.3657 - precision: 0.6415 - f1score: 0.4475\n",
      "Epoch 186/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4638 - acc: 0.7836 - recall: 0.3674 - precision: 0.6641 - f1score: 0.4519\n",
      "Epoch 187/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4625 - acc: 0.7790 - recall: 0.3625 - precision: 0.6409 - f1score: 0.4438\n",
      "Epoch 188/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4629 - acc: 0.7822 - recall: 0.3825 - precision: 0.6374 - f1score: 0.4583\n",
      "Epoch 189/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4595 - acc: 0.7829 - recall: 0.3661 - precision: 0.6582 - f1score: 0.4516\n",
      "Epoch 190/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4601 - acc: 0.7846 - recall: 0.3765 - precision: 0.6524 - f1score: 0.4609\n",
      "Epoch 191/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4605 - acc: 0.7841 - recall: 0.3701 - precision: 0.6519 - f1score: 0.4544\n",
      "Epoch 192/500\n",
      "8081/8081 [==============================] - 0s 55us/step - loss: 0.4587 - acc: 0.7848 - recall: 0.3806 - precision: 0.6535 - f1score: 0.4633\n",
      "Epoch 193/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4598 - acc: 0.7848 - recall: 0.3681 - precision: 0.6616 - f1score: 0.4554\n",
      "Epoch 194/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4603 - acc: 0.7843 - recall: 0.3794 - precision: 0.6438 - f1score: 0.4611\n",
      "Epoch 195/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4598 - acc: 0.7886 - recall: 0.3831 - precision: 0.6752 - f1score: 0.4680\n",
      "Epoch 196/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4647 - acc: 0.7784 - recall: 0.3678 - precision: 0.6320 - f1score: 0.4483\n",
      "Epoch 197/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4588 - acc: 0.7852 - recall: 0.3689 - precision: 0.6558 - f1score: 0.4543\n",
      "Epoch 198/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4624 - acc: 0.7844 - recall: 0.3838 - precision: 0.6475 - f1score: 0.4660\n",
      "Epoch 199/500\n",
      "8081/8081 [==============================] - 0s 55us/step - loss: 0.4637 - acc: 0.7828 - recall: 0.3783 - precision: 0.6447 - f1score: 0.4570\n",
      "Epoch 200/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4612 - acc: 0.7894 - recall: 0.4087 - precision: 0.6456 - f1score: 0.4838\n",
      "Epoch 201/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4619 - acc: 0.7818 - recall: 0.3532 - precision: 0.6499 - f1score: 0.4402\n",
      "Epoch 202/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4625 - acc: 0.7826 - recall: 0.3685 - precision: 0.6467 - f1score: 0.4534\n",
      "Epoch 203/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4614 - acc: 0.7853 - recall: 0.3840 - precision: 0.6515 - f1score: 0.4649\n",
      "Epoch 204/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4607 - acc: 0.7826 - recall: 0.3926 - precision: 0.6361 - f1score: 0.4698\n",
      "Epoch 205/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4638 - acc: 0.7817 - recall: 0.3618 - precision: 0.6379 - f1score: 0.4459\n",
      "Epoch 206/500\n",
      "8081/8081 [==============================] - 0s 55us/step - loss: 0.4648 - acc: 0.7817 - recall: 0.3665 - precision: 0.6431 - f1score: 0.4486\n",
      "Epoch 207/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4625 - acc: 0.7817 - recall: 0.3731 - precision: 0.6415 - f1score: 0.4549\n",
      "Epoch 208/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4624 - acc: 0.7848 - recall: 0.3693 - precision: 0.6557 - f1score: 0.4539\n",
      "Epoch 209/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4609 - acc: 0.7823 - recall: 0.3904 - precision: 0.6380 - f1score: 0.4682\n",
      "Epoch 210/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4614 - acc: 0.7878 - recall: 0.3778 - precision: 0.6601 - f1score: 0.4654\n",
      "Epoch 211/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4638 - acc: 0.7808 - recall: 0.3762 - precision: 0.6418 - f1score: 0.4558\n",
      "Epoch 212/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4612 - acc: 0.7821 - recall: 0.3627 - precision: 0.6513 - f1score: 0.4493\n",
      "Epoch 213/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4611 - acc: 0.7808 - recall: 0.3598 - precision: 0.6491 - f1score: 0.4446\n",
      "Epoch 214/500\n",
      "8081/8081 [==============================] - 0s 55us/step - loss: 0.4621 - acc: 0.7816 - recall: 0.3640 - precision: 0.6478 - f1score: 0.4460\n",
      "Epoch 215/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4604 - acc: 0.7854 - recall: 0.3813 - precision: 0.6527 - f1score: 0.4643\n",
      "Epoch 216/500\n",
      "8081/8081 [==============================] - 0s 55us/step - loss: 0.4628 - acc: 0.7836 - recall: 0.3731 - precision: 0.6415 - f1score: 0.4570\n",
      "Epoch 217/500\n",
      "8081/8081 [==============================] - 0s 55us/step - loss: 0.4615 - acc: 0.7834 - recall: 0.3694 - precision: 0.6526 - f1score: 0.4568\n",
      "Epoch 218/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4604 - acc: 0.7813 - recall: 0.3926 - precision: 0.6317 - f1score: 0.4636\n",
      "Epoch 219/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4593 - acc: 0.7813 - recall: 0.3676 - precision: 0.6430 - f1score: 0.4511\n",
      "Epoch 220/500\n",
      "8081/8081 [==============================] - 0s 55us/step - loss: 0.4627 - acc: 0.7826 - recall: 0.3706 - precision: 0.6386 - f1score: 0.4541\n",
      "Epoch 221/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4601 - acc: 0.7853 - recall: 0.3951 - precision: 0.6558 - f1score: 0.4741\n",
      "Epoch 222/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4599 - acc: 0.7829 - recall: 0.3838 - precision: 0.6403 - f1score: 0.4634\n",
      "Epoch 223/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4613 - acc: 0.7848 - recall: 0.3672 - precision: 0.6574 - f1score: 0.4550\n",
      "Epoch 224/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4614 - acc: 0.7800 - recall: 0.3681 - precision: 0.6386 - f1score: 0.4487\n",
      "Epoch 225/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4584 - acc: 0.7813 - recall: 0.3749 - precision: 0.6410 - f1score: 0.4564\n",
      "Epoch 226/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4613 - acc: 0.7791 - recall: 0.3749 - precision: 0.6290 - f1score: 0.4540\n",
      "Epoch 227/500\n",
      "8081/8081 [==============================] - 0s 55us/step - loss: 0.4620 - acc: 0.7813 - recall: 0.3594 - precision: 0.6469 - f1score: 0.4446\n",
      "Epoch 228/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4620 - acc: 0.7848 - recall: 0.3873 - precision: 0.6491 - f1score: 0.4660\n",
      "Epoch 229/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4608 - acc: 0.7815 - recall: 0.3692 - precision: 0.6500 - f1score: 0.4500\n",
      "Epoch 230/500\n",
      "8081/8081 [==============================] - 0s 55us/step - loss: 0.4607 - acc: 0.7808 - recall: 0.3736 - precision: 0.6337 - f1score: 0.4515\n",
      "Epoch 231/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4605 - acc: 0.7797 - recall: 0.3694 - precision: 0.6380 - f1score: 0.4482\n",
      "Epoch 232/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4597 - acc: 0.7818 - recall: 0.3699 - precision: 0.6429 - f1score: 0.4509\n",
      "Epoch 233/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4626 - acc: 0.7836 - recall: 0.3805 - precision: 0.6462 - f1score: 0.4603\n",
      "Epoch 234/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4621 - acc: 0.7803 - recall: 0.3854 - precision: 0.6243 - f1score: 0.4596\n",
      "Epoch 235/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4617 - acc: 0.7823 - recall: 0.3681 - precision: 0.6412 - f1score: 0.4541\n",
      "Epoch 236/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4624 - acc: 0.7836 - recall: 0.3750 - precision: 0.6387 - f1score: 0.4558\n",
      "Epoch 237/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4601 - acc: 0.7920 - recall: 0.4043 - precision: 0.6626 - f1score: 0.4869\n",
      "Epoch 238/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4604 - acc: 0.7833 - recall: 0.3790 - precision: 0.6503 - f1score: 0.4627\n",
      "Epoch 239/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4614 - acc: 0.7818 - recall: 0.3689 - precision: 0.6433 - f1score: 0.4508\n",
      "Epoch 240/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4631 - acc: 0.7820 - recall: 0.3784 - precision: 0.6339 - f1score: 0.4581\n",
      "Epoch 241/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4608 - acc: 0.7765 - recall: 0.3654 - precision: 0.6302 - f1score: 0.4403\n",
      "Epoch 242/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4586 - acc: 0.7818 - recall: 0.3804 - precision: 0.6492 - f1score: 0.4618\n",
      "Epoch 243/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4585 - acc: 0.7807 - recall: 0.3834 - precision: 0.6360 - f1score: 0.4617\n",
      "Epoch 244/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4624 - acc: 0.7805 - recall: 0.3802 - precision: 0.6340 - f1score: 0.4573\n",
      "Epoch 245/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4617 - acc: 0.7827 - recall: 0.3769 - precision: 0.6489 - f1score: 0.4612\n",
      "Epoch 246/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4603 - acc: 0.7841 - recall: 0.3692 - precision: 0.6487 - f1score: 0.4533\n",
      "Epoch 247/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4597 - acc: 0.7852 - recall: 0.3844 - precision: 0.6476 - f1score: 0.4677\n",
      "Epoch 248/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4604 - acc: 0.7869 - recall: 0.3845 - precision: 0.6551 - f1score: 0.4692\n",
      "Epoch 249/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4610 - acc: 0.7833 - recall: 0.3892 - precision: 0.6387 - f1score: 0.4680\n",
      "Epoch 250/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4622 - acc: 0.7801 - recall: 0.3690 - precision: 0.6432 - f1score: 0.4501\n",
      "Epoch 251/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4592 - acc: 0.7813 - recall: 0.3888 - precision: 0.6403 - f1score: 0.4678\n",
      "Epoch 252/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4595 - acc: 0.7822 - recall: 0.3908 - precision: 0.6324 - f1score: 0.4678\n",
      "Epoch 253/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4622 - acc: 0.7818 - recall: 0.3611 - precision: 0.6424 - f1score: 0.4441\n",
      "Epoch 254/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4610 - acc: 0.7791 - recall: 0.3692 - precision: 0.6298 - f1score: 0.4494\n",
      "Epoch 255/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4625 - acc: 0.7818 - recall: 0.3798 - precision: 0.6453 - f1score: 0.4582\n",
      "Epoch 256/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4610 - acc: 0.7841 - recall: 0.3828 - precision: 0.6473 - f1score: 0.4622\n",
      "Epoch 257/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4608 - acc: 0.7854 - recall: 0.3881 - precision: 0.6495 - f1score: 0.4665\n",
      "Epoch 258/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4601 - acc: 0.7843 - recall: 0.3853 - precision: 0.6479 - f1score: 0.4676\n",
      "Epoch 259/500\n",
      "8081/8081 [==============================] - 0s 51us/step - loss: 0.4624 - acc: 0.7816 - recall: 0.3782 - precision: 0.6500 - f1score: 0.4589\n",
      "Epoch 260/500\n",
      "8081/8081 [==============================] - 0s 51us/step - loss: 0.4595 - acc: 0.7839 - recall: 0.3837 - precision: 0.6429 - f1score: 0.4618\n",
      "Epoch 261/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4646 - acc: 0.7773 - recall: 0.3581 - precision: 0.6306 - f1score: 0.4377\n",
      "Epoch 262/500\n",
      "8081/8081 [==============================] - 0s 55us/step - loss: 0.4619 - acc: 0.7810 - recall: 0.3662 - precision: 0.6464 - f1score: 0.4491\n",
      "Epoch 263/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4620 - acc: 0.7815 - recall: 0.3708 - precision: 0.6414 - f1score: 0.4532\n",
      "Epoch 264/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4641 - acc: 0.7843 - recall: 0.3672 - precision: 0.6500 - f1score: 0.4528\n",
      "Epoch 265/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4641 - acc: 0.7818 - recall: 0.3803 - precision: 0.6389 - f1score: 0.4609\n",
      "Epoch 266/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4649 - acc: 0.7820 - recall: 0.3808 - precision: 0.6398 - f1score: 0.4613\n",
      "Epoch 267/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4616 - acc: 0.7813 - recall: 0.3656 - precision: 0.6520 - f1score: 0.4523\n",
      "Epoch 268/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4632 - acc: 0.7828 - recall: 0.3699 - precision: 0.6465 - f1score: 0.4543\n",
      "Epoch 269/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4596 - acc: 0.7848 - recall: 0.3843 - precision: 0.6541 - f1score: 0.4638\n",
      "Epoch 270/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4596 - acc: 0.7852 - recall: 0.3882 - precision: 0.6475 - f1score: 0.4682\n",
      "Epoch 271/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4611 - acc: 0.7834 - recall: 0.3838 - precision: 0.6498 - f1score: 0.4620\n",
      "Epoch 272/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4605 - acc: 0.7812 - recall: 0.3656 - precision: 0.6413 - f1score: 0.4472\n",
      "Epoch 273/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4618 - acc: 0.7843 - recall: 0.3873 - precision: 0.6429 - f1score: 0.4670\n",
      "Epoch 274/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4600 - acc: 0.7863 - recall: 0.3799 - precision: 0.6649 - f1score: 0.4646\n",
      "Epoch 275/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4621 - acc: 0.7836 - recall: 0.4006 - precision: 0.6419 - f1score: 0.4727\n",
      "Epoch 276/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4596 - acc: 0.7795 - recall: 0.3612 - precision: 0.6282 - f1score: 0.4441\n",
      "Epoch 277/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4588 - acc: 0.7826 - recall: 0.3826 - precision: 0.6366 - f1score: 0.4600\n",
      "Epoch 278/500\n",
      "8081/8081 [==============================] - 0s 55us/step - loss: 0.4606 - acc: 0.7849 - recall: 0.3919 - precision: 0.6442 - f1score: 0.4714\n",
      "Epoch 279/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4620 - acc: 0.7862 - recall: 0.3879 - precision: 0.6518 - f1score: 0.4674\n",
      "Epoch 280/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4612 - acc: 0.7829 - recall: 0.3662 - precision: 0.6467 - f1score: 0.4503\n",
      "Epoch 281/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4625 - acc: 0.7816 - recall: 0.3752 - precision: 0.6367 - f1score: 0.4542\n",
      "Epoch 282/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4604 - acc: 0.7854 - recall: 0.3805 - precision: 0.6573 - f1score: 0.4649\n",
      "Epoch 283/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4608 - acc: 0.7837 - recall: 0.3808 - precision: 0.6514 - f1score: 0.4643\n",
      "Epoch 284/500\n",
      "8081/8081 [==============================] - 0s 51us/step - loss: 0.4583 - acc: 0.7865 - recall: 0.3971 - precision: 0.6606 - f1score: 0.4745\n",
      "Epoch 285/500\n",
      "8081/8081 [==============================] - 0s 51us/step - loss: 0.4598 - acc: 0.7867 - recall: 0.3838 - precision: 0.6547 - f1score: 0.4683\n",
      "Epoch 286/500\n",
      "8081/8081 [==============================] - 0s 56us/step - loss: 0.4627 - acc: 0.7812 - recall: 0.3869 - precision: 0.6409 - f1score: 0.4631\n",
      "Epoch 287/500\n",
      "8081/8081 [==============================] - 0s 55us/step - loss: 0.4604 - acc: 0.7837 - recall: 0.3757 - precision: 0.6577 - f1score: 0.4598\n",
      "Epoch 288/500\n",
      "8081/8081 [==============================] - 0s 55us/step - loss: 0.4578 - acc: 0.7864 - recall: 0.3989 - precision: 0.6587 - f1score: 0.4778\n",
      "Epoch 289/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4590 - acc: 0.7905 - recall: 0.3986 - precision: 0.6607 - f1score: 0.4815\n",
      "Epoch 290/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4606 - acc: 0.7841 - recall: 0.3874 - precision: 0.6533 - f1score: 0.4664\n",
      "Epoch 291/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4595 - acc: 0.7832 - recall: 0.3886 - precision: 0.6479 - f1score: 0.4692\n",
      "Epoch 292/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4615 - acc: 0.7843 - recall: 0.3925 - precision: 0.6384 - f1score: 0.4663\n",
      "Epoch 293/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4623 - acc: 0.7825 - recall: 0.3631 - precision: 0.6445 - f1score: 0.4443\n",
      "Epoch 294/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4581 - acc: 0.7874 - recall: 0.4071 - precision: 0.6535 - f1score: 0.4829\n",
      "Epoch 295/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4616 - acc: 0.7858 - recall: 0.3932 - precision: 0.6500 - f1score: 0.4733\n",
      "Epoch 296/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4638 - acc: 0.7833 - recall: 0.3809 - precision: 0.6489 - f1score: 0.4603\n",
      "Epoch 297/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4628 - acc: 0.7864 - recall: 0.3883 - precision: 0.6606 - f1score: 0.4692\n",
      "Epoch 298/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4565 - acc: 0.7879 - recall: 0.3892 - precision: 0.6593 - f1score: 0.4719\n",
      "Epoch 299/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4604 - acc: 0.7832 - recall: 0.3840 - precision: 0.6469 - f1score: 0.4620\n",
      "Epoch 300/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4601 - acc: 0.7857 - recall: 0.3767 - precision: 0.6565 - f1score: 0.4641\n",
      "Epoch 301/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4606 - acc: 0.7841 - recall: 0.3810 - precision: 0.6489 - f1score: 0.4603\n",
      "Epoch 302/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4648 - acc: 0.7784 - recall: 0.3696 - precision: 0.6350 - f1score: 0.4434\n",
      "Epoch 303/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4609 - acc: 0.7868 - recall: 0.3798 - precision: 0.6635 - f1score: 0.4662\n",
      "Epoch 304/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4649 - acc: 0.7815 - recall: 0.3716 - precision: 0.6341 - f1score: 0.4508\n",
      "Epoch 305/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4598 - acc: 0.7849 - recall: 0.3908 - precision: 0.6505 - f1score: 0.4689\n",
      "Epoch 306/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4573 - acc: 0.7895 - recall: 0.3824 - precision: 0.6662 - f1score: 0.4697\n",
      "Epoch 307/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4626 - acc: 0.7837 - recall: 0.3738 - precision: 0.6522 - f1score: 0.4568\n",
      "Epoch 308/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4606 - acc: 0.7811 - recall: 0.3735 - precision: 0.6439 - f1score: 0.4555\n",
      "Epoch 309/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4599 - acc: 0.7899 - recall: 0.4078 - precision: 0.6629 - f1score: 0.4862\n",
      "Epoch 310/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4609 - acc: 0.7838 - recall: 0.3543 - precision: 0.6615 - f1score: 0.4460\n",
      "Epoch 311/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4615 - acc: 0.7878 - recall: 0.3976 - precision: 0.6529 - f1score: 0.4766\n",
      "Epoch 312/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4604 - acc: 0.7818 - recall: 0.3903 - precision: 0.6279 - f1score: 0.4663\n",
      "Epoch 313/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4611 - acc: 0.7855 - recall: 0.3810 - precision: 0.6523 - f1score: 0.4652\n",
      "Epoch 314/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4632 - acc: 0.7834 - recall: 0.3803 - precision: 0.6466 - f1score: 0.4607\n",
      "Epoch 315/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4582 - acc: 0.7877 - recall: 0.3816 - precision: 0.6649 - f1score: 0.4699\n",
      "Epoch 316/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4602 - acc: 0.7843 - recall: 0.3848 - precision: 0.6445 - f1score: 0.4633\n",
      "Epoch 317/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4609 - acc: 0.7865 - recall: 0.3810 - precision: 0.6538 - f1score: 0.4639\n",
      "Epoch 318/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4570 - acc: 0.7854 - recall: 0.3825 - precision: 0.6551 - f1score: 0.4661\n",
      "Epoch 319/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4587 - acc: 0.7811 - recall: 0.3659 - precision: 0.6386 - f1score: 0.4474\n",
      "Epoch 320/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4620 - acc: 0.7854 - recall: 0.3783 - precision: 0.6532 - f1score: 0.4598\n",
      "Epoch 321/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4600 - acc: 0.7867 - recall: 0.3754 - precision: 0.6542 - f1score: 0.4601\n",
      "Epoch 322/500\n",
      "8081/8081 [==============================] - 0s 56us/step - loss: 0.4620 - acc: 0.7829 - recall: 0.3928 - precision: 0.6426 - f1score: 0.4688\n",
      "Epoch 323/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4605 - acc: 0.7859 - recall: 0.3928 - precision: 0.6553 - f1score: 0.4699\n",
      "Epoch 324/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4588 - acc: 0.7872 - recall: 0.3801 - precision: 0.6565 - f1score: 0.4660\n",
      "Epoch 325/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4593 - acc: 0.7883 - recall: 0.3872 - precision: 0.6589 - f1score: 0.4691\n",
      "Epoch 326/500\n",
      "8081/8081 [==============================] - 0s 55us/step - loss: 0.4607 - acc: 0.7838 - recall: 0.3732 - precision: 0.6536 - f1score: 0.4548\n",
      "Epoch 327/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4593 - acc: 0.7847 - recall: 0.3857 - precision: 0.6483 - f1score: 0.4656\n",
      "Epoch 328/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4601 - acc: 0.7847 - recall: 0.3821 - precision: 0.6392 - f1score: 0.4646\n",
      "Epoch 329/500\n",
      "8081/8081 [==============================] - 0s 55us/step - loss: 0.4589 - acc: 0.7878 - recall: 0.3805 - precision: 0.6689 - f1score: 0.4680\n",
      "Epoch 330/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4591 - acc: 0.7864 - recall: 0.3962 - precision: 0.6479 - f1score: 0.4769\n",
      "Epoch 331/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4588 - acc: 0.7890 - recall: 0.3787 - precision: 0.6673 - f1score: 0.4674\n",
      "Epoch 332/500\n",
      "8081/8081 [==============================] - 0s 55us/step - loss: 0.4630 - acc: 0.7822 - recall: 0.3672 - precision: 0.6480 - f1score: 0.4530\n",
      "Epoch 333/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4609 - acc: 0.7843 - recall: 0.3866 - precision: 0.6400 - f1score: 0.4636\n",
      "Epoch 334/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4631 - acc: 0.7827 - recall: 0.3784 - precision: 0.6520 - f1score: 0.4583\n",
      "Epoch 335/500\n",
      "8081/8081 [==============================] - 0s 55us/step - loss: 0.4555 - acc: 0.7888 - recall: 0.3970 - precision: 0.6593 - f1score: 0.4812\n",
      "Epoch 336/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4638 - acc: 0.7810 - recall: 0.3641 - precision: 0.6386 - f1score: 0.4426\n",
      "Epoch 337/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4593 - acc: 0.7860 - recall: 0.3839 - precision: 0.6634 - f1score: 0.4669\n",
      "Epoch 338/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4595 - acc: 0.7846 - recall: 0.3774 - precision: 0.6519 - f1score: 0.4601\n",
      "Epoch 339/500\n",
      "8081/8081 [==============================] - 0s 55us/step - loss: 0.4605 - acc: 0.7852 - recall: 0.3786 - precision: 0.6553 - f1score: 0.4617\n",
      "Epoch 340/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4602 - acc: 0.7810 - recall: 0.3706 - precision: 0.6411 - f1score: 0.4508\n",
      "Epoch 341/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4586 - acc: 0.7874 - recall: 0.3925 - precision: 0.6530 - f1score: 0.4735\n",
      "Epoch 342/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4591 - acc: 0.7828 - recall: 0.3842 - precision: 0.6408 - f1score: 0.4640\n",
      "Epoch 343/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4562 - acc: 0.7853 - recall: 0.3875 - precision: 0.6536 - f1score: 0.4657\n",
      "Epoch 344/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4624 - acc: 0.7842 - recall: 0.3826 - precision: 0.6416 - f1score: 0.4632\n",
      "Epoch 345/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4614 - acc: 0.7829 - recall: 0.3845 - precision: 0.6393 - f1score: 0.4611\n",
      "Epoch 346/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4597 - acc: 0.7801 - recall: 0.3706 - precision: 0.6322 - f1score: 0.4517\n",
      "Epoch 347/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4615 - acc: 0.7848 - recall: 0.3663 - precision: 0.6598 - f1score: 0.4531\n",
      "Epoch 348/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4589 - acc: 0.7858 - recall: 0.3900 - precision: 0.6489 - f1score: 0.4713\n",
      "Epoch 349/500\n",
      "8081/8081 [==============================] - 0s 55us/step - loss: 0.4625 - acc: 0.7818 - recall: 0.3702 - precision: 0.6409 - f1score: 0.4537\n",
      "Epoch 350/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4617 - acc: 0.7836 - recall: 0.3798 - precision: 0.6459 - f1score: 0.4611\n",
      "Epoch 351/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4588 - acc: 0.7851 - recall: 0.3879 - precision: 0.6550 - f1score: 0.4676\n",
      "Epoch 352/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4604 - acc: 0.7853 - recall: 0.3901 - precision: 0.6527 - f1score: 0.4696\n",
      "Epoch 353/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4578 - acc: 0.7885 - recall: 0.3857 - precision: 0.6593 - f1score: 0.4708\n",
      "Epoch 354/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4621 - acc: 0.7760 - recall: 0.3541 - precision: 0.6215 - f1score: 0.4339\n",
      "Epoch 355/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4580 - acc: 0.7847 - recall: 0.3688 - precision: 0.6671 - f1score: 0.4570\n",
      "Epoch 356/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4612 - acc: 0.7808 - recall: 0.3726 - precision: 0.6361 - f1score: 0.4520\n",
      "Epoch 357/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4602 - acc: 0.7825 - recall: 0.3970 - precision: 0.6353 - f1score: 0.4708\n",
      "Epoch 358/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4580 - acc: 0.7853 - recall: 0.3886 - precision: 0.6513 - f1score: 0.4693\n",
      "Epoch 359/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4605 - acc: 0.7852 - recall: 0.3826 - precision: 0.6524 - f1score: 0.4652\n",
      "Epoch 360/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4628 - acc: 0.7808 - recall: 0.3631 - precision: 0.6453 - f1score: 0.4483\n",
      "Epoch 361/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4637 - acc: 0.7837 - recall: 0.3628 - precision: 0.6513 - f1score: 0.4456\n",
      "Epoch 362/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4583 - acc: 0.7829 - recall: 0.3692 - precision: 0.6505 - f1score: 0.4538\n",
      "Epoch 363/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4626 - acc: 0.7834 - recall: 0.3902 - precision: 0.6454 - f1score: 0.4683\n",
      "Epoch 364/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4603 - acc: 0.7872 - recall: 0.3787 - precision: 0.6635 - f1score: 0.4629\n",
      "Epoch 365/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4612 - acc: 0.7807 - recall: 0.3644 - precision: 0.6378 - f1score: 0.4463\n",
      "Epoch 366/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4586 - acc: 0.7852 - recall: 0.3894 - precision: 0.6559 - f1score: 0.4678\n",
      "Epoch 367/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4590 - acc: 0.7828 - recall: 0.3883 - precision: 0.6483 - f1score: 0.4641\n",
      "Epoch 368/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4634 - acc: 0.7812 - recall: 0.3606 - precision: 0.6454 - f1score: 0.4479\n",
      "Epoch 369/500\n",
      "8081/8081 [==============================] - 0s 55us/step - loss: 0.4602 - acc: 0.7836 - recall: 0.3901 - precision: 0.6496 - f1score: 0.4695\n",
      "Epoch 370/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4617 - acc: 0.7833 - recall: 0.3611 - precision: 0.6446 - f1score: 0.4459\n",
      "Epoch 371/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4604 - acc: 0.7811 - recall: 0.3718 - precision: 0.6376 - f1score: 0.4542\n",
      "Epoch 372/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4601 - acc: 0.7838 - recall: 0.3846 - precision: 0.6420 - f1score: 0.4637\n",
      "Epoch 373/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4619 - acc: 0.7858 - recall: 0.3721 - precision: 0.6509 - f1score: 0.4573\n",
      "Epoch 374/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4607 - acc: 0.7805 - recall: 0.3578 - precision: 0.6370 - f1score: 0.4439\n",
      "Epoch 375/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4605 - acc: 0.7817 - recall: 0.3786 - precision: 0.6426 - f1score: 0.4608\n",
      "Epoch 376/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4623 - acc: 0.7807 - recall: 0.3616 - precision: 0.6469 - f1score: 0.4457\n",
      "Epoch 377/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4617 - acc: 0.7837 - recall: 0.3726 - precision: 0.6488 - f1score: 0.4547\n",
      "Epoch 378/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4605 - acc: 0.7842 - recall: 0.3803 - precision: 0.6471 - f1score: 0.4585\n",
      "Epoch 379/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4608 - acc: 0.7839 - recall: 0.3811 - precision: 0.6459 - f1score: 0.4626\n",
      "Epoch 380/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4587 - acc: 0.7843 - recall: 0.3882 - precision: 0.6500 - f1score: 0.4687\n",
      "Epoch 381/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4594 - acc: 0.7873 - recall: 0.3890 - precision: 0.6584 - f1score: 0.4699\n",
      "Epoch 382/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4609 - acc: 0.7838 - recall: 0.3704 - precision: 0.6371 - f1score: 0.4536\n",
      "Epoch 383/500\n",
      "8081/8081 [==============================] - 0s 51us/step - loss: 0.4621 - acc: 0.7828 - recall: 0.3747 - precision: 0.6432 - f1score: 0.4555\n",
      "Epoch 384/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4593 - acc: 0.7837 - recall: 0.3839 - precision: 0.6466 - f1score: 0.4623\n",
      "Epoch 385/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4612 - acc: 0.7837 - recall: 0.3754 - precision: 0.6473 - f1score: 0.4591\n",
      "Epoch 386/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4629 - acc: 0.7813 - recall: 0.3545 - precision: 0.6577 - f1score: 0.4402\n",
      "Epoch 387/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4587 - acc: 0.7827 - recall: 0.3815 - precision: 0.6515 - f1score: 0.4627\n",
      "Epoch 388/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4630 - acc: 0.7791 - recall: 0.3514 - precision: 0.6406 - f1score: 0.4370\n",
      "Epoch 389/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4592 - acc: 0.7844 - recall: 0.3853 - precision: 0.6441 - f1score: 0.4666\n",
      "Epoch 390/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4605 - acc: 0.7829 - recall: 0.3780 - precision: 0.6409 - f1score: 0.4582\n",
      "Epoch 391/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4579 - acc: 0.7828 - recall: 0.3798 - precision: 0.6355 - f1score: 0.4589\n",
      "Epoch 392/500\n",
      "8081/8081 [==============================] - 0s 55us/step - loss: 0.4572 - acc: 0.7855 - recall: 0.3915 - precision: 0.6608 - f1score: 0.4733\n",
      "Epoch 393/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4609 - acc: 0.7837 - recall: 0.3818 - precision: 0.6424 - f1score: 0.4580\n",
      "Epoch 394/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4597 - acc: 0.7839 - recall: 0.3693 - precision: 0.6498 - f1score: 0.4537\n",
      "Epoch 395/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4577 - acc: 0.7853 - recall: 0.3909 - precision: 0.6469 - f1score: 0.4699\n",
      "Epoch 396/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4603 - acc: 0.7864 - recall: 0.3869 - precision: 0.6614 - f1score: 0.4661\n",
      "Epoch 397/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4602 - acc: 0.7800 - recall: 0.3718 - precision: 0.6311 - f1score: 0.4510\n",
      "Epoch 398/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4593 - acc: 0.7863 - recall: 0.3908 - precision: 0.6583 - f1score: 0.4715\n",
      "Epoch 399/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4632 - acc: 0.7834 - recall: 0.3739 - precision: 0.6482 - f1score: 0.4570\n",
      "Epoch 400/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4613 - acc: 0.7825 - recall: 0.3623 - precision: 0.6496 - f1score: 0.4494\n",
      "Epoch 401/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4626 - acc: 0.7828 - recall: 0.3734 - precision: 0.6498 - f1score: 0.4555\n",
      "Epoch 402/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4610 - acc: 0.7827 - recall: 0.3829 - precision: 0.6442 - f1score: 0.4602\n",
      "Epoch 403/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4630 - acc: 0.7834 - recall: 0.3848 - precision: 0.6387 - f1score: 0.4626\n",
      "Epoch 404/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4626 - acc: 0.7823 - recall: 0.3668 - precision: 0.6575 - f1score: 0.4462\n",
      "Epoch 405/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4615 - acc: 0.7813 - recall: 0.3789 - precision: 0.6310 - f1score: 0.4591\n",
      "Epoch 406/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4629 - acc: 0.7818 - recall: 0.3626 - precision: 0.6463 - f1score: 0.4455\n",
      "Epoch 407/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4571 - acc: 0.7880 - recall: 0.3995 - precision: 0.6546 - f1score: 0.4784\n",
      "Epoch 408/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4615 - acc: 0.7833 - recall: 0.3680 - precision: 0.6558 - f1score: 0.4526\n",
      "Epoch 409/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4599 - acc: 0.7816 - recall: 0.3878 - precision: 0.6419 - f1score: 0.4655\n",
      "Epoch 410/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4609 - acc: 0.7800 - recall: 0.3510 - precision: 0.6424 - f1score: 0.4367\n",
      "Epoch 411/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4615 - acc: 0.7829 - recall: 0.3898 - precision: 0.6409 - f1score: 0.4651\n",
      "Epoch 412/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4600 - acc: 0.7883 - recall: 0.3935 - precision: 0.6596 - f1score: 0.4753\n",
      "Epoch 413/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4585 - acc: 0.7854 - recall: 0.3709 - precision: 0.6605 - f1score: 0.4605\n",
      "Epoch 414/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4559 - acc: 0.7865 - recall: 0.4031 - precision: 0.6505 - f1score: 0.4805\n",
      "Epoch 415/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4611 - acc: 0.7795 - recall: 0.3757 - precision: 0.6428 - f1score: 0.4518\n",
      "Epoch 416/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4615 - acc: 0.7802 - recall: 0.3709 - precision: 0.6389 - f1score: 0.4507\n",
      "Epoch 417/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4585 - acc: 0.7808 - recall: 0.3855 - precision: 0.6447 - f1score: 0.4644\n",
      "Epoch 418/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4598 - acc: 0.7839 - recall: 0.3725 - precision: 0.6470 - f1score: 0.4580\n",
      "Epoch 419/500\n",
      "8081/8081 [==============================] - 0s 55us/step - loss: 0.4563 - acc: 0.7846 - recall: 0.4002 - precision: 0.6492 - f1score: 0.4771\n",
      "Epoch 420/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4596 - acc: 0.7857 - recall: 0.3760 - precision: 0.6542 - f1score: 0.4618\n",
      "Epoch 421/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4626 - acc: 0.7841 - recall: 0.3748 - precision: 0.6547 - f1score: 0.4564\n",
      "Epoch 422/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4575 - acc: 0.7853 - recall: 0.3769 - precision: 0.6584 - f1score: 0.4633\n",
      "Epoch 423/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4588 - acc: 0.7880 - recall: 0.3982 - precision: 0.6595 - f1score: 0.4772\n",
      "Epoch 424/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4607 - acc: 0.7865 - recall: 0.3883 - precision: 0.6530 - f1score: 0.4677\n",
      "Epoch 425/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4591 - acc: 0.7846 - recall: 0.3972 - precision: 0.6460 - f1score: 0.4728\n",
      "Epoch 426/500\n",
      "8081/8081 [==============================] - 0s 56us/step - loss: 0.4596 - acc: 0.7886 - recall: 0.4019 - precision: 0.6571 - f1score: 0.4817\n",
      "Epoch 427/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4602 - acc: 0.7817 - recall: 0.3578 - precision: 0.6518 - f1score: 0.4453\n",
      "Epoch 428/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4603 - acc: 0.7872 - recall: 0.3822 - precision: 0.6454 - f1score: 0.4651\n",
      "Epoch 429/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4617 - acc: 0.7834 - recall: 0.3784 - precision: 0.6400 - f1score: 0.4563\n",
      "Epoch 430/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4607 - acc: 0.7854 - recall: 0.3818 - precision: 0.6520 - f1score: 0.4644\n",
      "Epoch 431/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4612 - acc: 0.7852 - recall: 0.3758 - precision: 0.6479 - f1score: 0.4617\n",
      "Epoch 432/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4572 - acc: 0.7838 - recall: 0.3850 - precision: 0.6442 - f1score: 0.4636\n",
      "Epoch 433/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4589 - acc: 0.7862 - recall: 0.3693 - precision: 0.6580 - f1score: 0.4565\n",
      "Epoch 434/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4596 - acc: 0.7823 - recall: 0.3856 - precision: 0.6422 - f1score: 0.4618\n",
      "Epoch 435/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4597 - acc: 0.7854 - recall: 0.3637 - precision: 0.6556 - f1score: 0.4508\n",
      "Epoch 436/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4619 - acc: 0.7811 - recall: 0.3845 - precision: 0.6365 - f1score: 0.4617\n",
      "Epoch 437/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4603 - acc: 0.7808 - recall: 0.3599 - precision: 0.6586 - f1score: 0.4438\n",
      "Epoch 438/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4576 - acc: 0.7888 - recall: 0.4050 - precision: 0.6543 - f1score: 0.4835\n",
      "Epoch 439/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4614 - acc: 0.7818 - recall: 0.3700 - precision: 0.6495 - f1score: 0.4536\n",
      "Epoch 440/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4646 - acc: 0.7774 - recall: 0.3560 - precision: 0.6303 - f1score: 0.4338\n",
      "Epoch 441/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4600 - acc: 0.7834 - recall: 0.3706 - precision: 0.6536 - f1score: 0.4569\n",
      "Epoch 442/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4581 - acc: 0.7847 - recall: 0.3812 - precision: 0.6538 - f1score: 0.4660\n",
      "Epoch 443/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4570 - acc: 0.7803 - recall: 0.3773 - precision: 0.6323 - f1score: 0.4559\n",
      "Epoch 444/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4624 - acc: 0.7828 - recall: 0.3716 - precision: 0.6468 - f1score: 0.4531\n",
      "Epoch 445/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4584 - acc: 0.7849 - recall: 0.4037 - precision: 0.6417 - f1score: 0.4803\n",
      "Epoch 446/500\n",
      "8081/8081 [==============================] - 0s 51us/step - loss: 0.4634 - acc: 0.7776 - recall: 0.3599 - precision: 0.6265 - f1score: 0.4412\n",
      "Epoch 447/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4618 - acc: 0.7826 - recall: 0.3723 - precision: 0.6401 - f1score: 0.4534\n",
      "Epoch 448/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4605 - acc: 0.7821 - recall: 0.3801 - precision: 0.6492 - f1score: 0.4600\n",
      "Epoch 449/500\n",
      "8081/8081 [==============================] - 0s 55us/step - loss: 0.4602 - acc: 0.7802 - recall: 0.3745 - precision: 0.6375 - f1score: 0.4545\n",
      "Epoch 450/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4615 - acc: 0.7844 - recall: 0.3704 - precision: 0.6620 - f1score: 0.4552\n",
      "Epoch 451/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4596 - acc: 0.7838 - recall: 0.3897 - precision: 0.6435 - f1score: 0.4678\n",
      "Epoch 452/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4616 - acc: 0.7802 - recall: 0.3678 - precision: 0.6514 - f1score: 0.4511\n",
      "Epoch 453/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4600 - acc: 0.7829 - recall: 0.3770 - precision: 0.6406 - f1score: 0.4574\n",
      "Epoch 454/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4570 - acc: 0.7877 - recall: 0.4060 - precision: 0.6515 - f1score: 0.4849\n",
      "Epoch 455/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4586 - acc: 0.7880 - recall: 0.3842 - precision: 0.6657 - f1score: 0.4709\n",
      "Epoch 456/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4603 - acc: 0.7847 - recall: 0.3884 - precision: 0.6468 - f1score: 0.4667\n",
      "Epoch 457/500\n",
      "8081/8081 [==============================] - 0s 55us/step - loss: 0.4602 - acc: 0.7851 - recall: 0.3934 - precision: 0.6469 - f1score: 0.4710\n",
      "Epoch 458/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4604 - acc: 0.7842 - recall: 0.3734 - precision: 0.6545 - f1score: 0.4552\n",
      "Epoch 459/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4588 - acc: 0.7841 - recall: 0.3727 - precision: 0.6535 - f1score: 0.4578\n",
      "Epoch 460/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4589 - acc: 0.7841 - recall: 0.3775 - precision: 0.6465 - f1score: 0.4599\n",
      "Epoch 461/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4603 - acc: 0.7867 - recall: 0.3903 - precision: 0.6579 - f1score: 0.4715\n",
      "Epoch 462/500\n",
      "8081/8081 [==============================] - 0s 56us/step - loss: 0.4588 - acc: 0.7859 - recall: 0.3861 - precision: 0.6524 - f1score: 0.4682\n",
      "Epoch 463/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4588 - acc: 0.7853 - recall: 0.3794 - precision: 0.6512 - f1score: 0.4614\n",
      "Epoch 464/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4604 - acc: 0.7837 - recall: 0.3854 - precision: 0.6499 - f1score: 0.4641\n",
      "Epoch 465/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4610 - acc: 0.7836 - recall: 0.3769 - precision: 0.6514 - f1score: 0.4589\n",
      "Epoch 466/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4587 - acc: 0.7841 - recall: 0.3864 - precision: 0.6387 - f1score: 0.4649\n",
      "Epoch 467/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4614 - acc: 0.7875 - recall: 0.3783 - precision: 0.6647 - f1score: 0.4642\n",
      "Epoch 468/500\n",
      "8081/8081 [==============================] - 0s 51us/step - loss: 0.4586 - acc: 0.7867 - recall: 0.3818 - precision: 0.6602 - f1score: 0.4669\n",
      "Epoch 469/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4618 - acc: 0.7810 - recall: 0.3597 - precision: 0.6468 - f1score: 0.4453\n",
      "Epoch 470/500\n",
      "8081/8081 [==============================] - 0s 51us/step - loss: 0.4573 - acc: 0.7832 - recall: 0.3763 - precision: 0.6464 - f1score: 0.4612\n",
      "Epoch 471/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4605 - acc: 0.7881 - recall: 0.3908 - precision: 0.6633 - f1score: 0.4731\n",
      "Epoch 472/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4593 - acc: 0.7808 - recall: 0.3676 - precision: 0.6402 - f1score: 0.4522\n",
      "Epoch 473/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4609 - acc: 0.7831 - recall: 0.3734 - precision: 0.6399 - f1score: 0.4552\n",
      "Epoch 474/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4602 - acc: 0.7841 - recall: 0.3654 - precision: 0.6557 - f1score: 0.4515\n",
      "Epoch 475/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4613 - acc: 0.7849 - recall: 0.3850 - precision: 0.6568 - f1score: 0.4622\n",
      "Epoch 476/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4597 - acc: 0.7802 - recall: 0.3643 - precision: 0.6469 - f1score: 0.4476\n",
      "Epoch 477/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4590 - acc: 0.7864 - recall: 0.3835 - precision: 0.6532 - f1score: 0.4666\n",
      "Epoch 478/500\n",
      "8081/8081 [==============================] - 0s 55us/step - loss: 0.4627 - acc: 0.7825 - recall: 0.3714 - precision: 0.6460 - f1score: 0.4559\n",
      "Epoch 479/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4606 - acc: 0.7827 - recall: 0.3794 - precision: 0.6460 - f1score: 0.4591\n",
      "Epoch 480/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4623 - acc: 0.7813 - recall: 0.3683 - precision: 0.6435 - f1score: 0.4519\n",
      "Epoch 481/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4579 - acc: 0.7902 - recall: 0.3970 - precision: 0.6653 - f1score: 0.4826\n",
      "Epoch 482/500\n",
      "8081/8081 [==============================] - 0s 56us/step - loss: 0.4576 - acc: 0.7862 - recall: 0.3944 - precision: 0.6552 - f1score: 0.4741\n",
      "Epoch 483/500\n",
      "8081/8081 [==============================] - 0s 55us/step - loss: 0.4616 - acc: 0.7831 - recall: 0.3573 - precision: 0.6568 - f1score: 0.4433\n",
      "Epoch 484/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4598 - acc: 0.7867 - recall: 0.3858 - precision: 0.6554 - f1score: 0.4676\n",
      "Epoch 485/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4612 - acc: 0.7837 - recall: 0.3850 - precision: 0.6468 - f1score: 0.4640\n",
      "Epoch 486/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4619 - acc: 0.7820 - recall: 0.3695 - precision: 0.6369 - f1score: 0.4503\n",
      "Epoch 487/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4611 - acc: 0.7842 - recall: 0.3673 - precision: 0.6538 - f1score: 0.4514\n",
      "Epoch 488/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4622 - acc: 0.7868 - recall: 0.3863 - precision: 0.6523 - f1score: 0.4692\n",
      "Epoch 489/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4604 - acc: 0.7823 - recall: 0.3653 - precision: 0.6477 - f1score: 0.4513\n",
      "Epoch 490/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4591 - acc: 0.7846 - recall: 0.3699 - precision: 0.6498 - f1score: 0.4544\n",
      "Epoch 491/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4616 - acc: 0.7858 - recall: 0.3853 - precision: 0.6537 - f1score: 0.4671\n",
      "Epoch 492/500\n",
      "8081/8081 [==============================] - 0s 55us/step - loss: 0.4586 - acc: 0.7841 - recall: 0.3756 - precision: 0.6500 - f1score: 0.4583\n",
      "Epoch 493/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4607 - acc: 0.7842 - recall: 0.3809 - precision: 0.6470 - f1score: 0.4622\n",
      "Epoch 494/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4641 - acc: 0.7806 - recall: 0.3711 - precision: 0.6369 - f1score: 0.4527\n",
      "Epoch 495/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4604 - acc: 0.7828 - recall: 0.3551 - precision: 0.6538 - f1score: 0.4425\n",
      "Epoch 496/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4571 - acc: 0.7890 - recall: 0.4005 - precision: 0.6599 - f1score: 0.4798\n",
      "Epoch 497/500\n",
      "8081/8081 [==============================] - 0s 52us/step - loss: 0.4591 - acc: 0.7843 - recall: 0.3934 - precision: 0.6429 - f1score: 0.4713\n",
      "Epoch 498/500\n",
      "8081/8081 [==============================] - 0s 53us/step - loss: 0.4599 - acc: 0.7826 - recall: 0.3584 - precision: 0.6481 - f1score: 0.4469\n",
      "Epoch 499/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4566 - acc: 0.7868 - recall: 0.3818 - precision: 0.6595 - f1score: 0.4661\n",
      "Epoch 500/500\n",
      "8081/8081 [==============================] - 0s 54us/step - loss: 0.4615 - acc: 0.7813 - recall: 0.3703 - precision: 0.6440 - f1score: 0.4501\n"
     ]
    }
   ],
   "source": [
    "adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy', recall, precision, f1score])\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "\n",
    "hist = model.fit(x_train, y_train, epochs=500, class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "colab_type": "code",
    "id": "f3yax73D6L7A",
    "outputId": "906ad6c0-f059-4c97-d50b-7d4ee212ddd7"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaMAAAEKCAYAAAC/hjrSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3XmYFNWh//93VW+zywDDKgIiOa64\noiIuuEajxpgYjVFjXH/eaMRsXpOYRI03mqtEYzSJW8z3JvHG5GqMURMFBVzQiEgMIhxcQJZhmWFg\n9umt6vdHNTgwDM7W1Cyf1/Pw0FNdp/qcXs6n61T1Kcf3fURERMLkhl0BERERhZGIiIROYSQiIqFT\nGImISOgURiIiEjqFkYiIhC6az40bY/YH/grcZa29d7v7TgJ+AmSBZ621P85nXUREZMd6Q1+dtz0j\nY0wx8AvghXZWuQf4AjAVOMUYs2++6iIiIjvWW/rqfA7TJYHPAJXb32GM2ROosdaustZ6wLPAiXms\ni4iI7Fiv6KvzNkxnrc0AGWPMju4eAVS1+nsDMGFn2/N933ccp+cqKCIyMOy04+zpvrqr8nrMqBM+\nMWWqqxu6vPGKilKqquq7XL4vUpsHBrV5YOhOmysqSnuyKnnbIwjrbLpKgsTdYjQ72EUUEZFQ7bK+\nOpQwstauAMqMMeOMMVHgDOD5MOoiIiI7tiv76rwN0xljDgVmAOOAtDHmHOApYLm19i/AfwD/m1v9\nMWvtsnzVRUREdqy39NVOX7mERFVVfZcrqjHmgUFtHhjU5k6X7RNnfmkGBhERCZ3CSEREQqcwEhGR\n0CmMREQkdAqjbpozp73pnNr6+c9nUFm55hPXe+utN7nxxuu7Uy0RkT5FYdQNa9dWMmvWcx1ef/r0\nbzFq1Og81khEpG/qLdMB9Uk/+9lPWbJkMY888iCe51FZuYa1ayu5++5fctttt1BVtYHm5mYuvfRK\npk49hmuuuZJvfvN6Zs9+gcbGBlau/Ig1a1Zz7bXfYsqUqTt8jBdemMljj/2BSCSCMftw3XXfZtmy\npcyY8VNisRjxeJybb76NtWvXbLPsvvt+QR5n7hAR6VH9JozWrbuRurond3jf++87eF7nf6ZUVvY5\nRoy4td37zz//Ip544k9ccskVPPzw/WQyaX75y4fYtKmGww8/ktNOO4M1a1bzgx/cwNSpx2xTdsOG\n9dx55z28/vo8/vrXx3cYRk1NTTzwwH088sijFBUVcf313+Ctt97kpZdmc/bZ53DqqaezYMF8amo2\n8uyzf9tmWVVVFWVlwzrdZhGRMPSbMOoN9tlnPwBKS8tYsmQxTz31BI7jUldX22bdSZMOAmDYsGE0\nNOx4EthVq1ay++57UFRUBMDBBx/KsmVLOfro47jzzttZtWolJ554MmPHjmuzbMKECQPuh4Ei0nf1\nmzAaMeLWdvdidtUvtmOxGAAzZ/6Duro67rvvIerq6rj88ovarBuJRLbebm8WDMfZ9r5MJk0ikeCw\nww7noYf+h3nzXubWW2/immuua7Ps+9//LhMm7NeDrRMRyZ9+E0ZhcF2XbDbbZvnmzZsZOXIUrusy\nd+6LpNPpLm1/zJixrF69kqamRoqKilm48C0uvvgyHn/8MaZMOZpTTjkN3/dZtmwpy5d/sM2yJUuW\nKIxEpM9QGHXD2LHjsXYp99wzg+Likq3Lp007gRtu+CbvvvsOp5/+WYYNG8YjjzzY6e0XFhZy9dXT\n+da3vo7juEyadBAHHngQzc1N/OAHN1BSUkIsFuN73/sRy5bZbZbNmHEHfWTaQRERTZTaX6nNA4Pa\nPDBoolQREZFdQGEkIiKhUxiJiEjoFEYiIhI6hZGIiIROYSQiIqFTGImISOgURiIiEjqFkYiIhE5h\nJCIioVMYiYhI6BRGIiISOoWRiIiETmEkIiKhUxiJiEjoFEYiIhI6hZGIiIROYSQiIqFTGImISOgU\nRiIiEjqFkYiIhE5hJCIioYvmc+PGmLuAIwEfmG6tnd/qvquBC4Es8Ka19rp81kVERNrqLf103vaM\njDHHAROttVOAy4B7Wt1XBnwHOMZaezSwrzHmyHzVRURE2upN/XQ+h+lOBJ4EsNYuAcpzjQNI5f6V\nGGOiQBFQk8e6iIhIW72mn87nMN0IYEGrv6tyy+qstS3GmJuBD4Fm4I/W2mU729jQoSU4jtPlylRU\nlHa5bF+lNndfOg3Wwv779+hme5Re54EhT23u0X66O/J6zGg7W5Mkl7zfAz4F1AEvGmMOtNa+3V7h\n6uqGLj9wRUUpVVX1XS4fFt+HykqH0aP9TpfNR5uzWdiwwWHkyM7X55N4Hrid2E/3PPjtb2McfXSW\nCRM8brklwRlnxJk8eds2v/OOy6BBPrvv3rU633hjggceiPP4400cc0x2m/saG4PnpKysncKfwM9V\nafvvWNksVFU5DB/uc+21Baxd6/CnPzVTWwubNgXLX3opyvHHZxgzppT33qtn/XqXGTPiTJ2aZebM\nKOeem+Yzn8mwapVDIgHl5T6FhVBfDwUFMGtWlNJSn8mTszz7bJSFCyNceGGamhqH5csdzj8/A8D7\n7wePV7qDfvCNN1zWr3c588xMm/vWrnUoLfUpKdn5c9DcDNXVDmPGtP/6+H7wz3GCf63f20uXujQ2\nwqGHeljrUlnpMHVqlnh82228+GKEigqfl1+OcPTRWfbe2yMe3/Y1ePrpKAUFPiedlGVn3njDZfNm\nh5NOyuK6wXuxpQWKioLtzZ/vMnasT02Nw+bNDlOmBNvLZoP3+MMPx3jmmSgPP9xMaSnEYsHr0vo5\nttblz3+Ocu21KRwH9tyz65/nToZYt/rp7shnGFUSJOwWo4C1udv7AB9aa6sBjDEvA4cCeWlkZ6XT\nMH9+hEcfjXHTTUmGDm3/g/LiixFeey3Cd76TYubMKEcfnWG33YJtxGLBOs3N8OGHLp4HBxzgkc3C\nk09GGT/eY/x4j/LyYD3Pg+efj/Dqq1HefdeluNjnueeifPvbKVIp+OIXMzz6aIzTTstwxBEff2Ce\ney7Cu+9GmDIly0MPxaio8HngAWhqgmQSGhsdvvvdApqa4OSTM/zqV3EOPDBLaSmccUaGQw4JPlSR\niM8NNxRgrcsxx2R5880I112XZPHiCKNHe2zY4HLrrQnOOSfN2LEe11yTYsGC4EO+apXDggUR1qxx\nOe+8NA0NDnPnRigr82lqcjjhhAx77+0xZ06EZcsiXH11ioqK4Hm9774YN99cwODBHoMH+3z601ku\nuihFba3DRx+5VFcHgTx6tMcf/hCjpsZh7VqHN94I3r6u6+N5Dr/6FRx3XCHnnZdm8uQs119fwOzZ\nUYYO9Zg/v5EbbiggnYbPfz7NyJE+N96YoL7e4ZvfTHHaaRluvz3OXnt5nHtuhtmzIxQVwQMPBL3a\ngw/GOPTQoJNrboZf/zrO/ffHicd9nn22iVgMhg71eeyxGH/8Y4yxYz0mT87y5S+neeihGGPH+kya\nlOUXv4jz4YcuRx2V5bHHYowY4XHFFWluuSVBWZlPIgELFkQAiMd9Uqmgb7jggkJeeKHtx3XSJPj3\nvz/ubJ58MnjTPf98xz7aRUXB6wNBm7aYPh0qKjyqqoJvCKeemmbatCx//nOMjRsd9tsvyzPPBI/1\nhS+kGTPG48UXoxx4YJaRI31+/vM4yaTDxRenOOaYLHV1DosWubz5ZoTf/a6ZOXMiLF4c4bnnolRW\nOlx7bYqSEp+DD/ZYtcrhgw9c3n47QiIRBPNbbwXPyXXXJZk2DT74IMZBB2U5+eQiPM/hhRcaOffc\nQjZuDOr76quNzJwZ4a9/jbFhg8OaNW2/6Ywb57F2rUMy6XDccRnmzg2es3vuaWb+/Ah77umxZo3L\nueemqatzeOaZKEOH+txxRwKAUaM8jj8+w6xZUaqrHQ4/PMuoUT6PPx7b5nGuvjrF669HWLAgwkkn\nBesD7L13KaNHe4wb5/Hqq8Gy/fbLctxxWX7/+xh1dQ733JNg0CCfpUs792WtE3pNP+34fs9/ywUw\nxhwF3GytPdkYcwhwT+4gGMaY4cCrwAHW2mZjzEzgFmvty+1tr6qqvssVbW8vwfehpsbh9dcjjBnj\nsf/+HvffH+NHPypo1Y4s//mfKdavd3jllQi+D0OG+Lz1VoSGhqCzbM9ee2XZfXefOXM+7hh+/etm\nFi6McP/9wQd//HiPn/ykhVTK4fbb4yxZEulQm8rKfOrqHD772TRPPRVrc/9hh8E77wRPWSoFnrfz\nIc4tHfquUlLic9ppGc4+O82Xv1y0yx63I/bZJ7vT1yGR8Ekmna2vwZa/+5Jhw4IvFwCFhT7NzX2r\n/p+kvNxn06aP2xSJ+BQVQX19z7Rz+9d85EiPtWvzdwj+hz+Ea67p8p5Ru43u6X66O/IWRgDGmNuB\nYwEPuBo4GKi11v7FGPP/AZcAGWCetfb6nW2rJ8No1SqHt9+OcNVVBWSzkM0Gr1U06pPJhPehdF2f\nc87JcOGFaRYudPnTn2IsXhxh7FiPvff2WLHCwdrINt+Yt3f22Wn+8pe24dRaSYnPZz+bZvhwH2td\nnn02WL+iwuPCC4PhncsvLyQSgYkTPZ57rv1v2UcckeFTn/IYPtznsMOyrFrlcuONCYqK2KYzgOCb\n6MUXp1i4MMIbb0RYt27HH94tnfz48R5FRT6VlS6bNjmMGuVx001JJk3KUlYG3/hGAc89F+WKK1JM\nmODx/vsFDB2a5KOPXDZudCgo8Ckv93njjQiplMOnPpXlrLMyPP10lNpah1NOyfDaa5Gt7Y/Hg+G8\nDz8M6pVI+FxxRYpTTsly/fUJli4NAqq42OfSS1N84xsp7rgjwW9+E3xL/+c/o+yzT5YHHmjhJz+J\n8/e/B9sdO9Zj332zJJPBY06enOWnP03wuc+l+cMfYixcGOE3v2lm0aIIq1Y5XHRRmrVrHf7v/2Lc\nemuSxkZ4/fUomzbB5z+fYeVKlwkTPBYtcvnSl4Igv+CCFJdemuanP03whS+kt+6Nn3lmhgkTPN5+\nO8L998f4+tdT1Nc7nHBClvffdygvD75cLV/u8JvfxBk0yOellyKcf34wZDdzZpTzzkszZozPddcV\ncMghWYYNC17raBQuuaQQgNNPT3PSSVl+97sYU6dmGDLE5w9/iDFxokcq5VBe7pNKwerVLmvWOIwb\n5/G976UYNswjk3H45z8jVFU53H57sNcxdWqGL385zbhxwXvr0ksLyWTgnHPSFBcX4DgtzJ4dobQ0\n+PJw771x9t7b47e/beaEE4pZtcrlqKMyPPRQC74Pgwf7uC688kqEf/wjymc+k2HSpCzvvecya1aU\nWbOibNjgUFn58XvygAOyLFr08Ws+frzHunUON9yQ4qKL0jQ2Bu0pLfUZPdrfOkx44IFZvv/9AmIx\nWLnS4aijsqxc6fLMM1EuuCDNypXBnmBtrUNdncPixS6HHZalttbhxBOz3HhjgpUrXX784xbeeCPC\n669HuPLKGFOn9nwYQc/2092R1zDqST0RRu+953LBBYWsWPHxG27oUI+99vJ4/fWgwx00yOfKK1Oc\ne26au+6KM3t2lP32C4aX0mmHs85Kc/rpGd5+O8LkyVnq62HhwgiLF7uUl/vsu2+w233GGRmqqhxW\nrXI56KAsiQRMn17AE08Ewz1f/nIaa1322svj+uuDPbF7723m3HO3HX/f0bGUujp46aUo9fUwfrzP\nu++6HHFEltWrHU45Jct998XYtKmAK69s4Iknorz9doSbb04yZIhPdAe50tICiaAP2Hr8wvM+HqPP\nZuFPf4py331xzjgjw6JFET73uTTr1ztcckma4uJtt9fQAPF4UO9Fi1yWLnW3HoPYorkZvvSlQgoL\n4bLLUpx4YhbPCx43Hg+2UVz8cX1WrXIYNGjbYxd1dTBvXoRPfzrb5lhCZ2Sz8N57LuPHeyQSwfBm\nYeG2x3KSSbj66gKOPz7LBRektym/5TX64AOHUaOCYzMAjz4aJZkMnqP2pNPBc9HV406VlaVs3NjI\nAQd4XdtAJ2x/jKuhAb74xSIuvjjFl77U9rhRVx/jo48cxo3b9uPe+v24o9c5mw3uc93gfZFOOwwZ\n0vkuY+7cCP/6V2TrsZrf/S4I1COOCN5jW45d7WrdOQb8SWHUWwyoMNpyMBpg772zfP3rKb74xeBD\n9M9/RpgzJ/hGuMce+XlO0unggzZ+vE+k1SjQhx86vPxylIsuSvfYuHBfPWmjO9TmgUFt7nTZPhFG\nu/JsulD95CfxrUG0aFEDw4dvGzhHHJHd5qSAfIjFYK+92gbdnnv67Lln+9+eRUT6uwExN92KFXD3\n3cE41DHHZNoEkYiIhGtA7BlNnx78/7WvpbjqqlS4lRERkTb6fRj5PsyeDRMmePzoR8lQDj6KiMjO\n9fthuupqh/p6mDgxqyASEeml+n0YbfnNyJ576jiRiEhv1e/DaPnyYHdozz3z/zsMERHpmn4fRsOG\n+QwaxNbJCkVEpPfp9ycwnHBClpoaqK7WnpGISG/V7/eMIJzpO0REpOMGRBiJiEjvpjASEZHQKYxE\nRCR0CiMREQmdwkhEREKnMBIRkdApjEREJHQKIxERCZ3CSEREQqcwEhGR0CmMREQkdAojEREJncJI\nRERCpzASEZHQKYxERCR0CiMREQmdwkhEREKnMBIRkdApjEREJHQKIxERCZ3CSEREQqcwEhGR0CmM\nREQkdNF8btwYcxdwJOAD062181vdNwb4XyAOvGWtvSqfdRERkbZ6Sz+dtz0jY8xxwERr7RTgMuCe\n7VaZAcyw1h4OZI0xe+SrLiIi0lZv6qfzOUx3IvAkgLV2CVBujCkDMMa4wDHAU7n7r7bWrsxjXURE\npK1e00/nc5huBLCg1d9VuWV1QAVQD9xljDkEeNla+92dbWzo0BIcx+lyZSoqSrtctq9SmwcGtXlg\nyFObe7Sf7o68HjPajrPd7dHAz4EVwDPGmNOttc+0V7i6uqHLD1xRUUpVVX2Xy/dFavPAoDYPDN1p\ncydDrFv9dHfkc5iukiBhtxgFrM3drgY+stZ+YK3NAi8A++WxLiIi0lav6afzGUbPA+cA5HbxKq21\n9QDW2gzwoTFmYm7dQwGbx7qIiEhbvaafztswnbV2njFmgTFmHuABVxtjvgrUWmv/AlwH/DZ3kGwR\n8Ld81UVERNrqTf204/t+vrbdo6qq6rtcUY0xDwxq88CgNne6bNfP/OoEY8xVwB+27Fl1lmZgEBGR\nnjAJ+Lcx5v8ZY47pbGGFkYiIdJu19mvABOD/ARcYY+YZY643xpR3pLzCSEREeoS11gM+AFYTTCF0\nKPCyMeasTyq7K39nJCIi/ZQx5ivAJcBQ4EHgZGvtJmPMIGAu8NedlVcYiYhITzgF+KG19uUtC4wx\nu1lrNxtj7v6kwgojERHpNmvthcaYfY0xx+YWJQgmXt3HWvvIJ5VXGImISLfl9n4+TTCjw/sEJzPc\n2dHyOoFBRER6whHW2n2Af1lrJwMnA0UdLdzvw8j309TWvkZf+XGviEgflcz9nzDGONbaBcDUjhbu\ndBgZYxK5q//1CZs3/y8LFx5FU9O8sKsiItKfWWPM14CXgJnGmPuAQR0t3KFjRsaY7wINwMPAm0C9\nMeZ5a+0PulDhXSzI21RqOcXFHQ5pERHpnKuAcmAz8CVgOHBbRwt39ASGMwl2t74C/M1a+5/GmBc7\nWdFQRKNDAchkNoRcExGRfu0ua+11uduPdrZwR8Moba31jTGnEVxoCSDS2QcLQzQ6DIBstirkmoiI\n9GtZY8wJwDwgtWVhblaGT9TRY0abjTHPEJwv/pox5gyC6cZ7vS1hpD0jEZG8uhyYCTQBmdy/dEcL\nd3TP6MsEp+m9mvu7Bbi443UMTySyZZhOe0YiIvlird2tO+U7GkYVQJW1tsoYcwVwJJ34MVOYXLeA\nSGQ3hZGISB4ZY27Z0XJr7Q87Ur6jw3SPACljzMEEu2KPE0zz0CckEqNJpz/C81rCroqISH+VbfUv\nAhwPdHhvqaNh5Ftr5wNnA/daa58FdsnVA3vCkCGfwfMaaGh4IeyqiIj0S9bam1v9+z4wDRjb0fId\nDaMSY8xk4BzgH8aYBMH55H1CRcV5ANTW/hnf7xPnXYiI9HUxYK+OrtzRY0YzCK5PcX/uuNFtdOE8\n8rCUlh5KLDaOuronWL0axoz5bdhVEhHpV4wxq4DW864NBn7b0fIdCiNr7WPAY8aYwblLyH7PWttn\nJntzHIeKiu9QWXk1dXVPUF9/Ho6TwHEKcd0EBQUH4DixsKspItKXHd3qtg/UWWs3d7RwR6cDmgr8\nD1BKMLRXbYy50Fr7ZmdqGqby8osoKDiA5ctPZuXK87a5Lx6fwKBBFxCLjcbzmolGh1JT8yCeV4/n\nteA4UcaOfQrfbyISGZwLsuA3v6nUSmKxMThOnzmEJiKSD8XARdba7wIYYx4xxtxprV3ckcIdHaa7\nDTjLWvtO7kEOJpiJ4didluplCgsPYuzYx1m//haam9/YujyVWsGGDTs8K3Era8dtvR2LjSGR2BvX\nLaau7slt1isr+wKDBp1LNDqKmpqHcJwoFRXXk0p9QEPDi7S0/Jvy8q+QTq+lvv4ZUqkPiMX2IBYb\nTWHh4USjFXheI4MGfQmIkEwuJZvdRFHR4aTTa/G8BtLpjygsnEw0OgTfTwNRPK8O1y3GcbZ9SRsb\n51Fb+0c8L0l5+cUUFU3ZYXC2tCzFcRwSCdPuc5DJbCQSGYTjRMhkNuC6pbhu4U6ft47wvGYcp6BL\nge55TbjutrPU+75PNltNNFrR7bp1h+9ngAiO45DN1gEOkUhpjz6G56Xx/TSelyQSKenRbQ9Uvp/W\nSEnX3Ae0Po374dyyaR0p7HTk0grGmNnW2uO3W/aitfaEjteze6qq6rs8LFhRUUpVVf02yzyvhdra\nxygr+wLp9Co2bXqEaHQoDQ2zaWp6tZ0tuYQ18YTr7obn1W7923GKiMf3JJVahuMU4nm1OE6M0tIz\nSSYtUAsMIpl8p822IpFySkvPoKDgAOrqnqKlZdHWbRcXT6Ow8BAaGl6goOAAysrOAjxqah6koWFW\n7kfEWbLZTUSjIygomEQyuZTCwsNywVmN4xRQVHQ4vp8mmXyfwsIDc3uTMQoLDyadXkNNzcP4fjPx\nuKG5eQGQobDwCDyvgXh8D3bb7Xw8r5b6+udIpVbg+80kEnszdOg3iMVG47pFbNhwGzU1vyYeH4/r\nljB48NE0N6doaJhJOr2SkpKTSST2paHhOUpLzyCb3Ugms47S0s+QyWwEoKBgfzyvAd9PkUwuI5GY\nSDw+gcbGOaTTa3HdYgoLDyaZfI9UahnDht1ITc1viMfHUlw8jaam18lk1lFQsD8NDbMAl+Lio2lu\nfovq6ruIRkfiOAnS6RW55/d4hg+/iVTqIzKZNWSzmxk06CKaml4nmQy+QJaWnpFr+yyGDbuBbHYT\nzc1v09z8Jr6fprHxZcrLL6SoaCrr1l1FU9NiwGXYsBtzj38sqdQyMpmNNDcvYMiQrxGJ7EYq9RFN\nTa8TjY6gvPwC0ulKIpHBRCK70dT0GuvW3QhAOr2SiorrKS6eRjQ6DNctpKrqDmKx0ThOnJaWRSQS\nhpKSU2homEU0OoRYbAye18z69TeRzVZRUnIiRUVHAR4bN/6SsrLPkUjsTTq9FsdxSaWWU1g4maKi\nw0ilVtDYOBfHSdDQ8AKRSDnl5RdTXHw0LS3/JpHYj0xmLfX1z1Ff/wxFRcNpatqA65bhunGKio4l\nFhtFNruJkpKTyWZraG5+i9LSU/H9Fqqrf055+Vfx/TSx2Biam98kmXyPWGw4xcXH4ThFbN78ewoK\n9mfjxl/S1PRPxo17mk2bHqax8RUKCw+mrOws0unV+L5HWdkZOE6cdHoVkchQIpEyALLZTaTTq8hm\na/C8BmKx8UQig8hk1uF59bhuKTU1DxGJlDF48H/k3vc+mcw6wKew8HCy2U00NDxPMmnZY4//w3WL\naWp6hZEjD6O+vmu/Ka2oKN0lwzbGmJettcdst2yutfa4jpTvaBi9APySYKoHgFOBK6y1J3eyvl3W\n02G0M8EZd1kaG1+huHga6fRqIpESIpHy3LfQFnw/mQutWG4PxWPFik8DUFJyCun0RyQS+xOP70l1\n9R24bimJxKcoLj4e3w+mbRo06AJaWhZRWnoymzb9Dt9Pkcmspabmwa11CTrwI0mnVwLO1k4tEqkg\nm60iGh1JNlud20PqiC0nUG4fqhEKCibR0rKww89Texwn1on6dESUYGaRvsolEhlMNlvd+ZLbfQnJ\nB9ctwfMa2rnXYdtj0v2P4yTw/eQnr7hDbm40wsm95/PzZXX06GsZNOjWLpXdhWH0NPAMMIegozkV\nON5ae0ZHync0jCYCvwAOJ3hnvg583Vr7Ydeq3Xm7Moy6Kp1eTzQ6rM1wU2PjPAoK9iES6djZ8MGQ\nWYxotALXLd26Pd/PsGnT/1Baehqx2Eiy2VpctzT3zT5NQ8MsUqkVFBZOorCwidraDQwefCWe10BT\n0zxisfFEoxVEIoNoaHiBdHoNJSXTyGZrcN0y4vG92LDh1tw330+RSq2goGBvGhrmEI9PoKLi22ze\n/HtKSk4iGh2B6xaTTC4jFhvN5s3/S0HBPqTT6ykvv4impn/S0PBibgaMIRQWHgxE2LjxbuLxiQwe\nfCme10w8PhHfb6Km5iFqa5/AcRwKCw8jEgkug1JQcCDFxcfhukVs3HgvTU3zyWTW47oFRKOjaGp6\nhZKSkxg06CvstluMmpp1ZLObGDTofJqb/0VLy7+Jx8eTSn1IJlNNNDqETGYd9fUzicVGEI2OJJl8\nF9/3qKi4nmRyCen02q3bSKdX0ty8kERiLzKZGmpqHiSRmEhJyQkkk0spKJhEU9N8PK+WoqIpRKMV\nuT2ZBbhuGYMHX04sNnbrN/LNmx8lnV5JIrF37ht8EP6FhQdRVDSFVGoF9fVP4/spIpFympvfoqDg\nQGKxsaRSH+D7LZSVfZ7q6jtwnCLGjr2BdHokmzc/SmPjbOLxiRQVHUk8Ph7fT7Np08Nks3X4fkvu\n+TyYWGwUyeQSXHc3XLeAdHotkKGg4ADq6/8OQCQyhOLi42hpWUgqtRyA0tLP0tT0GonEBCBKU9Mr\nW9+zwRBxnKFDv0NV1X/lPgeJYCLaAAAPcElEQVTFZLObSCQm5vZAm3HdYly3gHh8As3NC2hsnEss\ntgee10g0Opxk0pJMvtvmM5FIGKLRYE+mvHwsyeRIHCfOihVn4ftNlJaeTiKxL9XVd7T7uYpGhxGP\nT6Cw8Aji8T2orr4791rsQzQ6gubmt4hGB5NI7Ed9/dMAxGLjGDr0G7S0/Jt0ejWZzHpaWv5FJDKY\n0tIzaW5eQDZbA3hEIkNw3SKam+dv87yUlp4OuCSTS4lEdqO5+U08rxGAIUOmU1g4iXR6PZs3P0oy\n+Q4lJafiebU0Nb2W24rDoYe+SUvLxA71H9vbhWFUQXBI5wiCnHgV+KG1tkPT3+w0jIwxL/Px16Lt\nG+Rba3fZMaO+EEa9idrcd/m+v/ULSDZbt3UYKPisBh+D+vqnKSk5keHDR1BVVY/ve6TTq4jHx+5w\nWy0tS4hGhxGNDtnpYzc0vEgyuYwhQ67auqypaT7gU1R0+Hbb9vD9ZG7PIIrvp3DdRPca36reyeTi\n3DCni+vuhuMEe/WtX+d0ej2+nyIeD673mUy+R2Pjq5SXX0g2W0tj42xKS0/D85raHEP0/TSp1HIS\niU8B5ALCxXULSSbfw3ULicV2b1O3lpZFxGJjtn5hak86vYZYbPQO25ZOLyca3R3XjbfT/jTJ5DKi\n0QrS6Ur22OOYLr+3d1UYQbDjYq19L3f7YGtth4daPimMdjrWZ62d2+FadpPCqHPU5oFBbR4YutPm\nXbhn9F/ASGvtpbm/HwOWW2tv6Ej5nZ5NtyvDRkRE+rRp1tqtl9O21p5njHllZwVa6+h0QCIiIjsT\nN8ZsHXc0xpQQTAnUIR39nZGIiMjO/BpYYox5k2DW7snA3R0trDASEZFus9Y+bIx5DxhKcKbNU8B3\ngbs6Ul5hJCIi3WaMuRv4NDACeB+YQCcuwqpjRiIi0hOOsNbuA/zLWjsZOBko+oQyWymMRESkJ2yZ\nxiJhjHGstQuAqTsr0Fpeh+mMMXcBRxKMH07PXS12+3VuA6ZYa6flsy4iItJWD/bT1hjzNeAlYKYx\nxgI7/2VwK3nbM8r9YHaitXYKcBlwzw7W2Zc+NvO3iEh/0cP99FXAH4HvAb8hOG50Zkfrks9huhOB\nJwGstUuAcmNM2XbrzAC+n8c6iIhI+3qsn7bW+tbaGmutZ6191Fp7l7V2dUcrks9huhHAglZ/V+WW\n1QEYY74KzAVWdGRjQ4eWdOsCdhUVPXsdmb5AbR4Y1OaBIU9t7tF+ujt25andW5PEGDMYuAQ4CWg7\nk+AOVFe3N8X9J9NcVgOD2jwwqM2dL9sJ3eqnuyOfw3SVBAm7xShgbe72CUAF8DLwF+CQ3EE0ERHZ\ndXpNP53PMHoeOAfAGHMIUGmtrQew1v6ftXZfa+2RwNnAW9bab+SxLiIi0lav6afzFkbW2nnAAmPM\nPIIzNK42xnzVGHN2vh5TREQ6rjf10x260mtvoOsZdY7aPDCozQNDX7ieUXdpBgYREQmdwkhEREKn\nMBIRkdApjEREJHQKIxERCZ3CSEREQqcwEhGR0CmMREQkdAojEREJncJIRERCpzASEZHQKYxERCR0\nCiMREQmdwkhEREKnMBIRkdApjEREJHQKIxERCZ3CSEREQqcwEhGR0CmMREQkdAojEREJncJIRERC\npzASEZHQKYxERCR0CiMREQmdwkhEREKnMBIRkdApjEREJHQKIxERCZ3CSEREQqcwEhGR0CmMREQk\ndAojEREJncJIRERCF83nxo0xdwFHAj4w3Vo7v9V9xwO3AVnAApdba7181kdERLbVW/rpvO0ZGWOO\nAyZaa6cAlwH3bLfKA8A51tqpQClwar7qIiIibfWmfjqfw3QnAk8CWGuXAOXGmLJW9x9qrV2du10F\nDMljXUREpK1e00/nc5huBLCg1d9VuWV1ANbaOgBjzEjgFOAHO9vY0KElOI7T5cpUVJR2uWxfpTYP\nDGrzwJCnNvdoP90deT1mtJ02SWKMGQb8DfiatXbjzgpXVzd0+YErKkqpqqrvcvm+SG0eGNTmgaE7\nbe5kiHWrn+6OfIZRJUHCbjEKWLvlj9yu4N+B71trn89jPUREZMd6TT+dz2NGzwPnABhjDgEqrbWt\no30GcJe19h95rIOIiLSv1/TTju/7edu4MeZ24FjAA64GDgZqgeeATcBrrVZ/1Fr7QHvbqqqq73JF\ntVs/MKjNA4Pa3OmyOz3Y3pP9dHfk9ZiRtfaG7Ra93ep2Ip+PLSIin6y39NOagUFEREKnMBIRkdAp\njEREJHQKIxERCZ3CSEREQqcwEhGR0CmMREQkdAojEREJncJIRERCpzASEZHQKYxERCR0CiMREQmd\nwkhEREKnMBIRkdApjEREJHQKIxERCZ3CSEREQqcwEhGR0CmMREQkdAojEREJncKom+bMeaHD6/78\n5zOorFyTx9qIiPRNCqNuWLu2klmznuvw+tOnf4tRo0bnsUYiIn1TNOwK9JSbbkrwt7/tuDmuC55X\n3OltnnlmhptuSrZ7/89+9lOWLFnMI488iOd5VFauYe3aSu6++5fcdtstVFVtoLm5mUsvvZKpU4/h\nmmuu5JvfvJ7Zs1+gsbGBlSs/Ys2a1Vx77beYMmXq1u1mMhn+679ualN+2bKlzJjxU1zXYf/9D+Tq\nq6fvcJmISF+jPaNuOP/8izjooEO45JIrAMhk0vzylw/R2NjA4Ycfyb33PsAtt9zGww/f36bshg3r\nufPOe5g+/ds89dQT29xXX1+3w/J3330n3/nO9/jVr35DTc1G1q1bu8NlIiJ9TT/aM0q2uxdTUVFK\nVVVj3uuwzz77AVBaWsaSJYt56qkncByXurraNutOmnQQAMOGDaOhoWGb+9orv3LlR+y110QAfvCD\nW9pdJiLS1/SbMOoNYrEYADNn/oO6ujruu+8h6urquPzyi9qsG4lEtt72fX+b+9or77ptd2R3tExE\npK9RT9YNruuSzWbbLN+8eTMjR47CdV3mzn2RdDrdqe22V37cuPEsXvwOALfddgsrVizf4TIRkb5G\nYdQNY8eOx9ql3HPPjG2WT5t2AvPmvcz06f9BYWEhw4YN45FHHuzwdtsrP336t7n33rv4j/+4jNLS\nMsaNG7/DZSIifY2z/RBRb1VVVd/ligbHjOp7sjq9nto8MKjNA0N32lxRUer0cHXyQntGIiISOoWR\niIiETmEkIiKhUxiJiEjoFEYiIhI6hZGIiIQurzMwGGPuAo4EfGC6tXZ+q/tOAn4CZIFnrbU/zmdd\nRESkrd7ST+dtz8gYcxww0Vo7BbgMuGe7Ve4BvgBMBU4xxuybr7qIiEhbvamfzucw3YnAkwDW2iVA\nuTGmDMAYsydQY61dZa31gGdz64uIyK7Ta/rpfA7TjQAWtPq7KresLvd/Vav7NgATdrax7v6KuKKi\ntDvF+yS1eWBQmweGPLW5R/vp7tiVJzDsLEz6xHQVIiL9XGj9dD7DqJIgWbcYBaxt577RuWUiIrLr\n9Jp+Op9h9DxwDoAx5hCg0lpbD2CtXQGUGWPGGWOiwBm59UVEZNfpNf10XmftNsbcDhwLeMDVwMFA\nrbX2L8aYY4Gf5lZ93Fp7Z94qIiIiO9Rb+uk+cwkJERHpvzQDg4iIhE5hJCIiocvrdEC9wc6muugP\njDH7A38F7rLW3muMGQP8DogQnBVzkbU2aYy5ALiOYFz4AWvtw6FVupuMMf8NHEPw/r0NmE8/bbMx\npgj4LTAcKAB+DLxNP21va8aYQuAdgja/QD9uszFmGvBnYHFu0SLgv+nHbd5ev94z6sBUF32aMaYY\n+AXBB3WLW4D7rLXHAO8Dl+bW+yFwEjAN+IYxZvAurm6PMMYcD+yfe01PBe6mf7f5TOBNa+1xwLnA\nz+jf7W3tRqAmd3sgtHmutXZa7t/XGRht3qpfhxE7meqin0gCn2Hbc/+nAU/lbv+N4E17BDDfWltr\nrW0GXiWYa6ovegn4Yu72ZqCYftxma+1j1tr/zv05BlhNP27vFsaYvYF9gWdyi6bRz9u8A9MYQG3u\n78N0O5vqos+z1maAjDGm9eJia20yd3sDMJIdT+sxcpdUsodZa7NAY+7Pywjmy/p0f24zgDFmHrA7\nwW89ZvX39gIzgGuAi3N/9+v3dc6+xpingMHAzQyMNm/V3/eMtjfQph1qr719/nkwxpxFEEbXbHdX\nv2yztfYo4LPA79m2Lf2uvcaYrwCvWWuXt7NKv2sz8B5BAJ1FEMAPs+3OQn9s8zb6exjtbKqL/qoh\nd+AXPp6+o19Nv2SM+TTwfeA0a20t/bjNxphDcyelYK39F0EHVd9f25tzOnCWMeZ14HLgB/Tj1xjA\nWrsmNyTrW2s/ANYRHFbot23eXn8Po3anuujHZhFcf4Tc//8A/glMNsYMMsaUEIwxvxxS/brFGLMb\ncAdwhrV2y8Ht/tzmY4FvARhjhgMl9O/2Yq09z1o72Vp7JPAQwdl0/brNxpgLjDHfzt0eQXD25CP0\n4zZvr9/PwLD9VBfW2rdDrlKPMcYcSjC2Pg5IA2uACwhOBS4APgIusdamjTHnAN8hOMX9F9baP4RR\n5+4yxlwJ3AQsa7X4YoJOq9+1OffN+GGCkxcKCYZy3gT+h37Y3u0ZY24CVgDP0Y/bbIwpBR4FBgFx\ngtd5If24zdvr92EkIiK9X38fphMRkT5AYSQiIqFTGImISOgURiIiEjqFkYiIhE5hJLILGGO+aoz5\nfdj1EOmtFEYiIhI6/c5IpBVjzNcJLtUQBZYSXFPmaeDvwIG51b5krV1jjDmdYDr/pty/K3PLjyC4\ntEWK4BIIXyH4Bf3nCSbp3ZfgR4yft9bqAyiC9oxEtjLGHA6cDRybu17SZoJp+/cEHsldV2YO8K3c\nRe8eAr5grT2eIKxuzW3q98AVuWsQzSWYaw1gP+BK4FBgf+CQXdEukb6gv19CQqQzpgF7AbNzl+Uo\nJpiIcqO1dsulSF4luMrmp4D11trVueVzgKuMMUOBQdbadwCstXdDcMyI4Do0Tbm/1xBM/SIiKIxE\nWksCT1lrt16WwhgzDnir1ToOwZxg2w+vtV7e3ohDZgdlRAQN04m09ipwWm42ZIwxXyO4cFm5Mebg\n3DpHA/8mmKh1mDFmj9zyk4DXrbUbgWpjzOTcNr6V246I7ITCSCTHWvsmcB8wxxjzCsGwXS3BbOhf\nNca8SDBl/125Sz5fBjxmjJlDcIn7G3Obugj4uTFmLsGM8TqlW+QT6Gw6kZ3IDdO9Yq3dPey6iPRn\n2jMSEZHQac9IRERCpz0jEREJncJIRERCpzASEZHQKYxERCR0CiMREQnd/w8clvLa/zEBDAAAAABJ\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f73d1a5a048>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5162/5162 [==============================] - 0s 57us/step\n",
      "loss_and_metrics : [0.49899591897541773, 0.7553273924835335, 0.3204356423185482, 0.6635178988499957, 0.41902422600132416]\n"
     ]
    }
   ],
   "source": [
    "# 학습과정 살펴보기\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.set_ylim([0.0, 1.0])\n",
    "acc_ax.set_ylim([0.0, 1.0])\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 모델 평가하기\n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=32)\n",
    "print('loss_and_metrics : ' + str(loss_and_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "  table {margin-left: 0 !important;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "  table {margin-left: 0 !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5CJKctKC6L7F"
   },
   "source": [
    "## Threshold 설정 및 결과\n",
    "\n",
    "test set 1 (2017년 9월 ~ 2018년 9월)\n",
    "\n",
    "| Threshold | Precision | 예측값=1의 비율 |\n",
    "| - - - - - | - - - - - | - - - - - - - |\n",
    "|  0.6  | 0.77    | 0.04  |\n",
    "|  0.5  | 0.68    | 0.13  |\n",
    "|  0.4  | 0.59    | 0.22  |\n",
    "\n",
    "test set 2 (2018년 3월 ~ 2018년 9월)\n",
    "\n",
    "| Threshold | Precision | 예측값=1의 비율 |\n",
    "| - - - - - | - - - - - | - - - - - - |\n",
    "|  0.6  |  0.75 |  0.03 |\n",
    "|  0.5  |  0.68 |  0.07 |\n",
    "|  0.4  |  0.59 |  0.13 |\n",
    "\n",
    "예측값이 1이 되는 경우의 비율도 고려해야 함. 1~3등을 예측하는 것이라 20~30%가 적절\n",
    "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "- 적절한 threshold에서 precision은 50%를 크게 벗어나지 않아 모델의 성능이 높지 않음\n",
    "- 모델 파라미터 설정에 따라 overfitting 또는 underfitting이 쉽게 일어나 적절한 파라미터를 찾기 어려움\n",
    "- 현 모델은 또한 경기 맥락을 충분히 고려하지 못한다는 한계를 가짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "kPObsPwe6L7J",
    "outputId": "c1aca814-9f10-4687-f480-092ef3a132a3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23266175900813638"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.5970024979184013"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = model.predict(x_test)\n",
    "yt = np.array(y_test)\n",
    "predictions = [0 for x in range(pred.shape[0])]\n",
    "for i, predicted in enumerate(pred):\n",
    "    if predicted[0] >= 0.4:\n",
    "        predictions[i] = 1\n",
    "    else:\n",
    "        predictions[i] = 0\n",
    "        \n",
    "df = pd.DataFrame({'prediction' : predictions, 'real' : yt})\n",
    "df['accurate'] = df['prediction'] == df['real']\n",
    "\n",
    "display(\n",
    "    df[df['prediction'] == 1].shape[0] / df.shape[0],\n",
    "    df[df['prediction'] == 1].accurate.mean()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NN_classification.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
